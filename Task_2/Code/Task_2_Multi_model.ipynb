{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10399957,"sourceType":"datasetVersion","datasetId":6443852},{"sourceId":10401145,"sourceType":"datasetVersion","datasetId":6444772},{"sourceId":10419045,"sourceType":"datasetVersion","datasetId":6457498}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download/Upload Data","metadata":{"id":"MkK8tOHiLw3I"}},{"cell_type":"markdown","source":"# 2. Setting up the enviroment","metadata":{"id":"mh7dFCORL0eR"}},{"cell_type":"code","source":"# Tabular Data Analysis\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Utility\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"id":"1dWgrRIOL1fp","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:07.727557Z","iopub.execute_input":"2025-01-10T10:14:07.727863Z","iopub.status.idle":"2025-01-10T10:14:09.007798Z","shell.execute_reply.started":"2025-01-10T10:14:07.727830Z","shell.execute_reply":"2025-01-10T10:14:09.007006Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# 4. Load the dataset","metadata":{"id":"k18aD2SaL40a"}},{"cell_type":"code","source":"all_df = pd.read_csv(\"/kaggle/input/dataset3/fake_news_classification_mal_train(3).csv\")\nall_df.head(3)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"5vymL2NfL53g","outputId":"0a145d39-2779-4c3c-8cfe-f186518f9694","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:09.008608Z","iopub.execute_input":"2025-01-10T10:14:09.008988Z","iopub.status.idle":"2025-01-10T10:14:09.051132Z","shell.execute_reply.started":"2025-01-10T10:14:09.008954Z","shell.execute_reply":"2025-01-10T10:14:09.050272Z"}},"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                 ID                                               News  \\\n0  FAKE_MAL_TR_0001  കേള്‍വി തകരാറുള്ള കുട്ടികള്‍ക്ക് നടത്തുന്ന സൗജ...   \n1  FAKE_MAL_TR_0002  ചന്ദ്രയാന് കേരള മുഖ്യമന്ത്രി പിണറായി വിജയൻ മാത...   \n2  FAKE_MAL_TR_0003  പിണറായി വിജയന്‍ സര്‍ക്കാര്‍ നിര്‍മിച്ച കേരളത്ത...   \n\n        Label  \n0  FALSE       \n1  FALSE       \n2  FALSE       ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>News</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FAKE_MAL_TR_0001</td>\n      <td>കേള്‍വി തകരാറുള്ള കുട്ടികള്‍ക്ക് നടത്തുന്ന സൗജ...</td>\n      <td>FALSE</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FAKE_MAL_TR_0002</td>\n      <td>ചന്ദ്രയാന് കേരള മുഖ്യമന്ത്രി പിണറായി വിജയൻ മാത...</td>\n      <td>FALSE</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FAKE_MAL_TR_0003</td>\n      <td>പിണറായി വിജയന്‍ സര്‍ക്കാര്‍ നിര്‍മിച്ച കേരളത്ത...</td>\n      <td>FALSE</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split into train, validation\ntrain_df, val_df = train_test_split(all_df, test_size=0.15, stratify=all_df[\"Label\"], random_state=42)","metadata":{"id":"wXAgTZ89n832","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:09.051939Z","iopub.execute_input":"2025-01-10T10:14:09.052251Z","iopub.status.idle":"2025-01-10T10:14:09.192643Z","shell.execute_reply.started":"2025-01-10T10:14:09.052228Z","shell.execute_reply":"2025-01-10T10:14:09.191802Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"#val_df = pd.read_csv(\"/content/CIOL-Winter-ML-Bootcamp/datasets/ResearchWriting/o1/val.csv\")\n#val_df.head(3)","metadata":{"id":"ELZfAk5akbXr","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:09.193646Z","iopub.execute_input":"2025-01-10T10:14:09.193977Z","iopub.status.idle":"2025-01-10T10:14:09.197554Z","shell.execute_reply.started":"2025-01-10T10:14:09.193946Z","shell.execute_reply":"2025-01-10T10:14:09.196673Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/dataset3/fake_news_classification_mal_test.xlsx - Sheet1.csv\")\ntest_df.head(3)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":143},"id":"2E_VKeHrOV6G","outputId":"048eb25f-6095-4888-8d54-223787869569","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:09.199357Z","iopub.execute_input":"2025-01-10T10:14:09.199580Z","iopub.status.idle":"2025-01-10T10:14:09.226011Z","shell.execute_reply.started":"2025-01-10T10:14:09.199560Z","shell.execute_reply":"2025-01-10T10:14:09.225216Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"               S.no                                               News\n0  FAKE_MAL_TE_0001  കേരളത്തില്‍ പുരുഷന്മാര്‍ക്ക് രണ്ട് ഭാര്യമാര്‍ ...\n1  FAKE_MAL_TE_0002  പാർട്ടിയുടെ കൊടിക്ക് മഹത്വം ഉണ്ടെന്നും സംരംഭങ്...\n2  FAKE_MAL_TE_0003  നവകേരള സദസ്സ്: കാട്ടാക്കട ക്രിസ്ത്യൻ കോളേജ് കവ...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>S.no</th>\n      <th>News</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>FAKE_MAL_TE_0001</td>\n      <td>കേരളത്തില്‍ പുരുഷന്മാര്‍ക്ക് രണ്ട് ഭാര്യമാര്‍ ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>FAKE_MAL_TE_0002</td>\n      <td>പാർട്ടിയുടെ കൊടിക്ക് മഹത്വം ഉണ്ടെന്നും സംരംഭങ്...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>FAKE_MAL_TE_0003</td>\n      <td>നവകേരള സദസ്സ്: കാട്ടാക്കട ക്രിസ്ത്യൻ കോളേജ് കവ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"TEXT_VAR = \"News\"\nLABEL_VAR = \"Label\"","metadata":{"id":"vBV05Fncp8BN","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:09.227472Z","iopub.execute_input":"2025-01-10T10:14:09.227825Z","iopub.status.idle":"2025-01-10T10:14:09.231461Z","shell.execute_reply.started":"2025-01-10T10:14:09.227791Z","shell.execute_reply":"2025-01-10T10:14:09.230561Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"Labels are not numerical. Let's make them numerical.","metadata":{"id":"pWPhww7yS-IM"}},{"cell_type":"code","source":"# Map text labels to numerical values\nlabel_mapping = {label: idx for idx, label in enumerate(train_df[LABEL_VAR].unique())}\ntrain_df[LABEL_VAR] = train_df[LABEL_VAR].map(label_mapping)  # Change as necessary\nval_df[LABEL_VAR] = val_df[LABEL_VAR].map(label_mapping)  # Change as necessary","metadata":{"id":"-OFVXk3HPXj2","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:09.232322Z","iopub.execute_input":"2025-01-10T10:14:09.232558Z","iopub.status.idle":"2025-01-10T10:14:09.250544Z","shell.execute_reply.started":"2025-01-10T10:14:09.232539Z","shell.execute_reply":"2025-01-10T10:14:09.249674Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"# Modeling","metadata":{"id":"DiD1U4XjudBB"}},{"cell_type":"markdown","source":"## Load Things","metadata":{"id":"e47rxAVBOFx3"}},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport pandas as pd\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoModel, AutoTokenizer, AutoProcessor\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\nfrom tqdm import tqdm","metadata":{"id":"6rnkckw3nPMn","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:09.251506Z","iopub.execute_input":"2025-01-10T10:14:09.251833Z","iopub.status.idle":"2025-01-10T10:14:20.639375Z","shell.execute_reply.started":"2025-01-10T10:14:09.251801Z","shell.execute_reply":"2025-01-10T10:14:20.638453Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"id":"XZ9DGFwlncqP","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:20.640175Z","iopub.execute_input":"2025-01-10T10:14:20.640671Z","iopub.status.idle":"2025-01-10T10:14:20.713746Z","shell.execute_reply.started":"2025-01-10T10:14:20.640647Z","shell.execute_reply":"2025-01-10T10:14:20.712663Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"seed = 42\ntorch.manual_seed(seed)","metadata":{"id":"K10J2skvHanJ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"369e9163-c31b-4654-c2c3-95c479d8d87a","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:20.714795Z","iopub.execute_input":"2025-01-10T10:14:20.715138Z","iopub.status.idle":"2025-01-10T10:14:20.734769Z","shell.execute_reply.started":"2025-01-10T10:14:20.715102Z","shell.execute_reply":"2025-01-10T10:14:20.733943Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7cbdd3e73e50>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Hyperparameters\nbatch_size = 8\n\nmodel_names = [\"l3cube-pune/malayalam-topic-all-doc\", \"mdosama39/malayalam-bert-FakeNews-Dravidian-finalwithPP\"]  # Add your model names here\nmax_lengths = [512, 512]  # Add the corresponding max lengths for each model. If max_length value is bigger than actual model's max_length, it'll show error.","metadata":{"id":"TzwK6-Jgn9OX","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:20.735827Z","iopub.execute_input":"2025-01-10T10:14:20.736152Z","iopub.status.idle":"2025-01-10T10:14:20.739519Z","shell.execute_reply.started":"2025-01-10T10:14:20.736111Z","shell.execute_reply":"2025-01-10T10:14:20.738793Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Collect Embeddings","metadata":{"id":"SvDAb6ndrsB0"}},{"cell_type":"code","source":"# Function to extract and concatenate embeddings from multiple models\ndef extract_text_embeddings_multiple_models(df, save_path, model_names, max_lengths):\n    if os.path.exists(save_path):\n        print(f\"Embeddings already exist at {save_path}\")\n        return torch.load(save_path)\n\n    # Initialize models and tokenizers once\n    models = [AutoModel.from_pretrained(model_name).to(device).eval() for model_name in model_names]\n    tokenizers = [AutoTokenizer.from_pretrained(model_name) for model_name in model_names]\n\n    all_embeddings = {}\n    with torch.no_grad():\n        for idx, row in tqdm(df.iterrows(), desc=\"Extracting text embeddings\", total=len(df)):\n            transcription = row[TEXT_VAR]\n            transcription = transcription if isinstance(transcription, str) else \"\"\n\n            model_embeddings = []\n\n            # Loop through each model and its corresponding tokenizer and max_length\n            for model, tokenizer, max_len in zip(models, tokenizers, max_lengths):\n                # Tokenize the text\n                inputs = tokenizer(\n                    transcription, padding=\"max_length\", truncation=True, max_length=max_len, return_tensors=\"pt\"\n                )\n                inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU/CPU\n\n                # Extract embeddings\n                outputs = model(**inputs)\n                cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token embeddings\n                model_embeddings.append(cls_embedding.cpu())\n\n            # Concatenate embeddings from all models\n            all_embeddings[idx] = torch.cat(model_embeddings, dim=1)  # Concatenate along feature dimension\n\n    torch.save(all_embeddings, save_path)\n    return all_embeddings","metadata":{"id":"FHrgied3qYUE","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:20.740252Z","iopub.execute_input":"2025-01-10T10:14:20.740500Z","iopub.status.idle":"2025-01-10T10:14:20.751498Z","shell.execute_reply.started":"2025-01-10T10:14:20.740478Z","shell.execute_reply":"2025-01-10T10:14:20.750754Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Collect and save embeddings for all models\ntrain_save_path = \"train_text_embeddings__\" + \"__\".join([f\"{model_name.split('/')[-1]}_{max_len}\" for model_name, max_len in zip(model_names, max_lengths)]) + \".pt\"\ntrain_text_embeddings = extract_text_embeddings_multiple_models(\n    train_df, train_save_path, model_names, max_lengths\n)\n\n# Collect and save embeddings for all models\nval_save_path = \"val_text_embeddings__\" + \"__\".join([f\"{model_name.split('/')[-1]}_{max_len}\" for model_name, max_len in zip(model_names, max_lengths)]) + \".pt\"\nval_text_embeddings = extract_text_embeddings_multiple_models(\n    val_df, val_save_path, model_names, max_lengths\n)\n\n# Collect and save embeddings for all models\ntest_save_path = \"test_text_embeddings__\" + \"__\".join([f\"{model_name.split('/')[-1]}_{max_len}\" for model_name, max_len in zip(model_names, max_lengths)]) + \".pt\"\ntest_text_embeddings = extract_text_embeddings_multiple_models(\n    test_df, test_save_path, model_names, max_lengths\n)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2jyZbtkObYjT","outputId":"acee9bea-785d-4391-e4c7-e1241c5e2075","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:14:20.752264Z","iopub.execute_input":"2025-01-10T10:14:20.752471Z","iopub.status.idle":"2025-01-10T10:17:30.436277Z","shell.execute_reply.started":"2025-01-10T10:14:20.752454Z","shell.execute_reply":"2025-01-10T10:17:30.435564Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.18k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66776f97905c43b496ea331ed4fb4a1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/950M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a81fc4829b94e03af2332635f226423"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/881 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"685c281057a54b98956472ecd543d035"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/950M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce3192fd49840a487210c906822f322"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.46k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"649180aeb53242c4bfa112a1b72f73b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6289b204ebde4b969b432c0d9f8dd662"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d628e6736ed9475a83f38760755325fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"523cb3c56d7540329feae50e7933fd3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"344b077efa2440498b754ea5cdc252f1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/3.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86896895bbbb4be3816ec719cdd2c955"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/6.41M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb789c5a09d4a35940b66c6b3271e61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"388563e0a20b49c18059fad507a11878"}},"metadata":{}},{"name":"stderr","text":"Extracting text embeddings: 100%|██████████| 1615/1615 [01:42<00:00, 15.70it/s]\nExtracting text embeddings: 100%|██████████| 285/285 [00:19<00:00, 14.42it/s]\nExtracting text embeddings: 100%|██████████| 200/200 [00:14<00:00, 14.18it/s]\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"## Load Embeddings","metadata":{"id":"LKT81NDSse1T"}},{"cell_type":"code","source":"def load_embeddings(embedding_path):\n    if os.path.exists(embedding_path):\n        print(f\"Loading embeddings from {embedding_path}\")\n        return torch.load(embedding_path)\n    else:\n        raise FileNotFoundError(f\"Embeddings file not found at {embedding_path}\")","metadata":{"id":"f19wvaWMsktS","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:17:30.437834Z","iopub.execute_input":"2025-01-10T10:17:30.438057Z","iopub.status.idle":"2025-01-10T10:17:30.442212Z","shell.execute_reply.started":"2025-01-10T10:17:30.438038Z","shell.execute_reply":"2025-01-10T10:17:30.441230Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_text_embeddings = load_embeddings(train_save_path)\nval_text_embeddings = load_embeddings(val_save_path)\ntest_text_embeddings = load_embeddings(test_save_path)","metadata":{"id":"OUb9t8sjrUCM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2ab620dd-879a-444b-de01-685f2ffb8196","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:17:30.445562Z","iopub.execute_input":"2025-01-10T10:17:30.445813Z","iopub.status.idle":"2025-01-10T10:17:30.557148Z","shell.execute_reply.started":"2025-01-10T10:17:30.445791Z","shell.execute_reply":"2025-01-10T10:17:30.556214Z"}},"outputs":[{"name":"stdout","text":"Loading embeddings from train_text_embeddings__malayalam-topic-all-doc_512__malayalam-bert-FakeNews-Dravidian-finalwithPP_512.pt\nLoading embeddings from val_text_embeddings__malayalam-topic-all-doc_512__malayalam-bert-FakeNews-Dravidian-finalwithPP_512.pt\nLoading embeddings from test_text_embeddings__malayalam-topic-all-doc_512__malayalam-bert-FakeNews-Dravidian-finalwithPP_512.pt\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Modeling","metadata":{"id":"Pvc6Dm5kshIj"}},{"cell_type":"code","source":"import torch.optim as optim\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"id":"z0Tb_r_irT27","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:17:30.558379Z","iopub.execute_input":"2025-01-10T10:17:30.558600Z","iopub.status.idle":"2025-01-10T10:17:30.562120Z","shell.execute_reply.started":"2025-01-10T10:17:30.558579Z","shell.execute_reply":"2025-01-10T10:17:30.561326Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def prepare_text_embeddings(text_embeddings, df, LABEL_VAR, has_labels=True):\n    combined_embeddings = []\n    labels = [] if has_labels else None\n\n    for idx, row in df.iterrows():\n        # Ensure the index exists in the text embeddings\n        if idx in text_embeddings:\n            text_embedding = text_embeddings[idx].squeeze()  # Squeeze to remove unnecessary dimensions\n\n            # Add the text embedding to the list\n            combined_embeddings.append(text_embedding)\n\n            if has_labels:\n                labels.append(row[LABEL_VAR])  # Get the label from the DataFrame\n\n    if has_labels:\n        return torch.stack(combined_embeddings), torch.tensor(labels)\n    else:\n        return torch.stack(combined_embeddings)","metadata":{"id":"zyDdzX-AtsbZ","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:17:30.562935Z","iopub.execute_input":"2025-01-10T10:17:30.563223Z","iopub.status.idle":"2025-01-10T10:17:30.576308Z","shell.execute_reply.started":"2025-01-10T10:17:30.563192Z","shell.execute_reply":"2025-01-10T10:17:30.575549Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"X_train, y_train = prepare_text_embeddings(train_text_embeddings, train_df, LABEL_VAR)\nX_val, y_val = prepare_text_embeddings(val_text_embeddings, val_df, LABEL_VAR)\nX_test = prepare_text_embeddings(test_text_embeddings, test_df, LABEL_VAR, has_labels=False)\n\nprint(f\"Training data shape: {X_train.shape}, Labels: {y_train.shape}\")\nprint(f\"Validation data shape: {X_val.shape}, Labels: {y_val.shape}\")\nprint(f\"Test data shape: {X_test.shape}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lemZ0ItCtvqx","outputId":"2fb61fce-af64-416f-c3e1-46625c3814b3","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:17:30.577035Z","iopub.execute_input":"2025-01-10T10:17:30.577240Z","iopub.status.idle":"2025-01-10T10:17:30.704270Z","shell.execute_reply.started":"2025-01-10T10:17:30.577214Z","shell.execute_reply":"2025-01-10T10:17:30.703462Z"}},"outputs":[{"name":"stdout","text":"Training data shape: torch.Size([1615, 1536]), Labels: torch.Size([1615])\nValidation data shape: torch.Size([285, 1536]), Labels: torch.Size([285])\nTest data shape: torch.Size([200, 1536])\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"from collections import Counter\ndef new_balance_classes(X, y):\n\n  # Load embeddings from .pt file\n#     embeddings = torch.load(embeddings_path)\n\n  # Count occurrences of each class\n  train_df=pd.DataFrame({\"Label\":y})\n  class_counts = Counter(train_df['Label'])\n\n\n\n  # Find the maximum count\n  max_count = max(class_counts.values())\n\n  # Create a new list for balanced data\n  label_lst = []\n  embedding_lst=[]\n\n\n  for idx, row in train_df.iterrows():\n      label = row[LABEL_VAR]\n      embedding = X[idx]  # Assuming embeddings are in the same order as DataFrame\n\n      # Add original sample\n      embedding_lst.append(embedding)  # Duplicate sample\n      label_lst.append(label)\n\n\n\n      # Duplicate samples for minority classes\n      duplicates_needed = int( (max_count - class_counts[label]) / class_counts[label])\n      for _ in range(duplicates_needed):\n          embedding_lst.append(embedding)  # Duplicate sample\n          label_lst.append(label)\n\n\n  return torch.stack(embedding_lst), np.array(label_lst)\n\n\n# # train_df.head()\nX_train,y_train=new_balance_classes(X_train,y_train)\n\nX_train = torch.tensor(X_train)\ny_train = torch.tensor(y_train)","metadata":{"id":"5OzBa-GcrWDn","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:17:30.704988Z","iopub.execute_input":"2025-01-10T10:17:30.705215Z","iopub.status.idle":"2025-01-10T10:17:30.798618Z","shell.execute_reply.started":"2025-01-10T10:17:30.705195Z","shell.execute_reply":"2025-01-10T10:17:30.797626Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Define the MLP model\nclass MLPModel(nn.Module):\n    def __init__(self, input_dim, hidden_dim, output_dim, dropout_p=0.5):\n        \"\"\"\n        Initialize the MLP model.\n        Args:\n            input_dim (int): Dimension of the input features.\n            hidden_dim (list of int): List of dimensions for hidden layers.\n            output_dim (int): Dimension of the output layer.\n            dropout_p (float): Dropout probability.\n        \"\"\"\n        super(MLPModel, self).__init__()\n        self.fc1 = nn.Linear(input_dim, hidden_dim[0])\n        self.relu = nn.ReLU()\n        self.dropout1 = nn.Dropout(p=dropout_p)\n        self.fc2 = nn.Linear(hidden_dim[0], hidden_dim[1])\n        self.dropout2 = nn.Dropout(p=dropout_p)\n        self.fc3 = nn.Linear(hidden_dim[1], output_dim)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout1(x)\n        x = self.relu(self.fc2(x))\n        x = self.dropout2(x)\n        x = self.fc3(x)\n        return x","metadata":{"id":"kG2eSLEzuDQJ","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:17:30.799685Z","iopub.execute_input":"2025-01-10T10:17:30.800024Z","iopub.status.idle":"2025-01-10T10:17:30.805815Z","shell.execute_reply.started":"2025-01-10T10:17:30.799999Z","shell.execute_reply":"2025-01-10T10:17:30.805095Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# Hyperparameters\ninput_dim = X_train.shape[1]\nnum_classes = len(train_df[LABEL_VAR].unique())\nhidden_dim = [786, 512]\noutput_dim = num_classes\nbatch_size = 16\nnum_epochs = 500\nlearning_rate = 0.0001\ndropout_p = 0.5","metadata":{"id":"mT_Yy49YuGXK","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:22:11.980893Z","iopub.execute_input":"2025-01-10T10:22:11.981228Z","iopub.status.idle":"2025-01-10T10:22:11.986051Z","shell.execute_reply.started":"2025-01-10T10:22:11.981204Z","shell.execute_reply":"2025-01-10T10:22:11.985224Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# Prepare the data loaders\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset = TensorDataset(X_val, y_val)\ntest_dataset = TensorDataset(X_test)\n\ntrain_loader = DataLoader(train_dataset, batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size)\ntest_loader = DataLoader(test_dataset, batch_size)","metadata":{"id":"nkOVzPn1uLyp","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:22:12.478562Z","iopub.execute_input":"2025-01-10T10:22:12.478892Z","iopub.status.idle":"2025-01-10T10:22:12.483386Z","shell.execute_reply.started":"2025-01-10T10:22:12.478865Z","shell.execute_reply":"2025-01-10T10:22:12.482502Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"# Initialize model, loss function, and optimizer\nmodel = MLPModel(input_dim, hidden_dim, output_dim, dropout_p).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)","metadata":{"id":"zeI8r1aTuOWM","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:22:12.985282Z","iopub.execute_input":"2025-01-10T10:22:12.985577Z","iopub.status.idle":"2025-01-10T10:22:13.002910Z","shell.execute_reply.started":"2025-01-10T10:22:12.985554Z","shell.execute_reply":"2025-01-10T10:22:13.002207Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# Function to calculate metrics\ndef calculate_metrics(preds, labels):\n    accuracy = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"macro\")\n    return accuracy, precision, recall, f1","metadata":{"id":"bYP39h-txZOp","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:22:15.625955Z","iopub.execute_input":"2025-01-10T10:22:15.626282Z","iopub.status.idle":"2025-01-10T10:22:15.630535Z","shell.execute_reply.started":"2025-01-10T10:22:15.626254Z","shell.execute_reply":"2025-01-10T10:22:15.629624Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Train and Val","metadata":{"id":"xCjg-mNzuZxJ"}},{"cell_type":"code","source":"# Train and save best model\ndef train_and_save_best_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, save_dir):\n    best_f1 = -float('inf')\n    best_model_path = None\n\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        all_train_preds, all_train_labels = [], []\n\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(inputs).squeeze()\n\n            # Compute loss and backpropagate\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n            _, preds = torch.max(outputs, dim=1)\n            all_train_preds.extend(preds.cpu().tolist())\n            all_train_labels.extend(labels.cpu().tolist())\n\n        # Calculate training metrics\n        train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(all_train_preds, all_train_labels)\n\n        # Validation phase\n        model.eval()\n        val_loss = 0\n        all_val_preds, all_val_labels = [], []\n\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs).squeeze()\n\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n\n                _, preds = torch.max(outputs, dim=1)\n                all_val_preds.extend(preds.cpu().tolist())\n                all_val_labels.extend(labels.cpu().tolist())\n\n        # Calculate validation metrics\n        val_accuracy, val_precision, val_recall, val_f1 = calculate_metrics(all_val_preds, all_val_labels)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss/len(train_loader):.4f}, \"\n              f\"Train Acc: {train_accuracy:.4f}, Prec: {train_precision:.4f}, Rec: {train_recall:.4f}, F1: {train_f1:.4f} | \"\n              f\"Val Loss: {val_loss/len(val_loader):.4f}, Val Acc: {val_accuracy:.4f}, Prec: {val_precision:.4f}, \"\n              f\"Rec: {val_recall:.4f}, F1: {val_f1:.4f}\")\n\n        # Save the model if it has the best F1 score on validation\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            best_model_path = f\"{save_dir}/best_model_epoch_{epoch + 1}_f1_{val_f1:.4f}.pth\"\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"Best model saved with F1: {val_f1:.4f} at epoch {epoch + 1}\")\n\n    return best_model_path","metadata":{"id":"1qFE7USauREx","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:22:16.640416Z","iopub.execute_input":"2025-01-10T10:22:16.640769Z","iopub.status.idle":"2025-01-10T10:22:16.649320Z","shell.execute_reply.started":"2025-01-10T10:22:16.640740Z","shell.execute_reply":"2025-01-10T10:22:16.648374Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Set the directory where the best model will be saved\nsave_dir = \"./models\"\nos.makedirs(save_dir, exist_ok=True)","metadata":{"id":"QpEjhQXkuTYq","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:22:19.768098Z","iopub.execute_input":"2025-01-10T10:22:19.768523Z","iopub.status.idle":"2025-01-10T10:22:19.772583Z","shell.execute_reply.started":"2025-01-10T10:22:19.768487Z","shell.execute_reply":"2025-01-10T10:22:19.771807Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"# Train the model and save the best model\nbest_model_path = train_and_save_best_model(\n    model=model,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    criterion=criterion,\n    optimizer=optimizer,\n    num_epochs=num_epochs,\n    save_dir=save_dir\n)\n\nprint(f\"Best model saved at: {best_model_path}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UI2TuU_nxehv","outputId":"c7bc19fc-b784-4cc3-dd36-f457c7536f4e","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:22:20.198735Z","iopub.execute_input":"2025-01-10T10:22:20.199034Z","iopub.status.idle":"2025-01-10T10:28:18.164524Z","shell.execute_reply.started":"2025-01-10T10:22:20.199011Z","shell.execute_reply":"2025-01-10T10:28:18.163569Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/500: Train Loss: 1.5358, Train Acc: 0.3163, Prec: 0.3088, Rec: 0.3142, F1: 0.2570 | Val Loss: 1.4685, Val Acc: 0.4877, Prec: 0.2476, Rec: 0.3679, F1: 0.2649\nBest model saved with F1: 0.2649 at epoch 1\nEpoch 2/500: Train Loss: 1.4505, Train Acc: 0.3796, Prec: 0.3558, Rec: 0.3780, F1: 0.3327 | Val Loss: 1.4076, Val Acc: 0.4772, Prec: 0.2856, Rec: 0.3819, F1: 0.2805\nBest model saved with F1: 0.2805 at epoch 2\nEpoch 3/500: Train Loss: 1.3976, Train Acc: 0.4020, Prec: 0.3708, Rec: 0.4004, F1: 0.3613 | Val Loss: 1.3770, Val Acc: 0.4456, Prec: 0.2725, Rec: 0.3583, F1: 0.2700\nEpoch 4/500: Train Loss: 1.3569, Train Acc: 0.4260, Prec: 0.3949, Rec: 0.4247, F1: 0.3865 | Val Loss: 1.4276, Val Acc: 0.3965, Prec: 0.2690, Rec: 0.3653, F1: 0.2557\nEpoch 5/500: Train Loss: 1.3094, Train Acc: 0.4598, Prec: 0.4314, Rec: 0.4587, F1: 0.4226 | Val Loss: 1.3181, Val Acc: 0.4982, Prec: 0.2939, Rec: 0.3712, F1: 0.2792\nEpoch 6/500: Train Loss: 1.2628, Train Acc: 0.4674, Prec: 0.4308, Rec: 0.4665, F1: 0.4294 | Val Loss: 1.3779, Val Acc: 0.4632, Prec: 0.2631, Rec: 0.3773, F1: 0.2722\nEpoch 7/500: Train Loss: 1.2246, Train Acc: 0.4916, Prec: 0.4623, Rec: 0.4907, F1: 0.4597 | Val Loss: 1.3470, Val Acc: 0.4246, Prec: 0.2682, Rec: 0.3459, F1: 0.2693\nEpoch 8/500: Train Loss: 1.1797, Train Acc: 0.5088, Prec: 0.4787, Rec: 0.5080, F1: 0.4808 | Val Loss: 1.4069, Val Acc: 0.3930, Prec: 0.2618, Rec: 0.3709, F1: 0.2612\nEpoch 9/500: Train Loss: 1.1477, Train Acc: 0.5156, Prec: 0.4862, Rec: 0.5150, F1: 0.4885 | Val Loss: 1.3459, Val Acc: 0.4281, Prec: 0.2677, Rec: 0.3614, F1: 0.2738\nEpoch 10/500: Train Loss: 1.1132, Train Acc: 0.5356, Prec: 0.5058, Rec: 0.5349, F1: 0.5088 | Val Loss: 1.3727, Val Acc: 0.4000, Prec: 0.2397, Rec: 0.3205, F1: 0.2404\nEpoch 11/500: Train Loss: 1.0809, Train Acc: 0.5498, Prec: 0.5213, Rec: 0.5490, F1: 0.5225 | Val Loss: 1.4138, Val Acc: 0.3614, Prec: 0.2671, Rec: 0.3257, F1: 0.2488\nEpoch 12/500: Train Loss: 1.0477, Train Acc: 0.5582, Prec: 0.5329, Rec: 0.5578, F1: 0.5368 | Val Loss: 1.3611, Val Acc: 0.3965, Prec: 0.2814, Rec: 0.3411, F1: 0.2684\nEpoch 13/500: Train Loss: 1.0282, Train Acc: 0.5668, Prec: 0.5422, Rec: 0.5664, F1: 0.5454 | Val Loss: 1.3068, Val Acc: 0.4491, Prec: 0.3059, Rec: 0.3668, F1: 0.3105\nBest model saved with F1: 0.3105 at epoch 13\nEpoch 14/500: Train Loss: 1.0083, Train Acc: 0.5778, Prec: 0.5541, Rec: 0.5772, F1: 0.5574 | Val Loss: 1.2908, Val Acc: 0.4667, Prec: 0.3047, Rec: 0.3582, F1: 0.3101\nEpoch 15/500: Train Loss: 0.9864, Train Acc: 0.5838, Prec: 0.5618, Rec: 0.5835, F1: 0.5647 | Val Loss: 1.3647, Val Acc: 0.4105, Prec: 0.3095, Rec: 0.3898, F1: 0.3114\nBest model saved with F1: 0.3114 at epoch 15\nEpoch 16/500: Train Loss: 0.9656, Train Acc: 0.5878, Prec: 0.5683, Rec: 0.5878, F1: 0.5721 | Val Loss: 1.3349, Val Acc: 0.4421, Prec: 0.2745, Rec: 0.3471, F1: 0.2756\nEpoch 17/500: Train Loss: 0.9451, Train Acc: 0.5996, Prec: 0.5797, Rec: 0.5993, F1: 0.5813 | Val Loss: 1.2968, Val Acc: 0.4702, Prec: 0.3008, Rec: 0.3596, F1: 0.3099\nEpoch 18/500: Train Loss: 0.9254, Train Acc: 0.6072, Prec: 0.5900, Rec: 0.6072, F1: 0.5926 | Val Loss: 1.3121, Val Acc: 0.4526, Prec: 0.3036, Rec: 0.3610, F1: 0.3030\nEpoch 19/500: Train Loss: 0.9022, Train Acc: 0.6182, Prec: 0.6010, Rec: 0.6183, F1: 0.6031 | Val Loss: 1.2575, Val Acc: 0.5158, Prec: 0.3292, Rec: 0.3915, F1: 0.3334\nBest model saved with F1: 0.3334 at epoch 19\nEpoch 20/500: Train Loss: 0.8937, Train Acc: 0.6257, Prec: 0.6098, Rec: 0.6260, F1: 0.6117 | Val Loss: 1.3683, Val Acc: 0.4175, Prec: 0.3001, Rec: 0.3414, F1: 0.2912\nEpoch 21/500: Train Loss: 0.8619, Train Acc: 0.6287, Prec: 0.6107, Rec: 0.6291, F1: 0.6137 | Val Loss: 1.3834, Val Acc: 0.4070, Prec: 0.3067, Rec: 0.3512, F1: 0.2922\nEpoch 22/500: Train Loss: 0.8563, Train Acc: 0.6405, Prec: 0.6235, Rec: 0.6410, F1: 0.6260 | Val Loss: 1.3496, Val Acc: 0.4596, Prec: 0.3267, Rec: 0.3877, F1: 0.3323\nEpoch 23/500: Train Loss: 0.8394, Train Acc: 0.6533, Prec: 0.6382, Rec: 0.6539, F1: 0.6402 | Val Loss: 1.3278, Val Acc: 0.4456, Prec: 0.2998, Rec: 0.3381, F1: 0.3035\nEpoch 24/500: Train Loss: 0.8235, Train Acc: 0.6521, Prec: 0.6366, Rec: 0.6526, F1: 0.6390 | Val Loss: 1.3068, Val Acc: 0.4596, Prec: 0.2993, Rec: 0.3643, F1: 0.3017\nEpoch 25/500: Train Loss: 0.8144, Train Acc: 0.6581, Prec: 0.6436, Rec: 0.6589, F1: 0.6453 | Val Loss: 1.3147, Val Acc: 0.4877, Prec: 0.3202, Rec: 0.3688, F1: 0.3240\nEpoch 26/500: Train Loss: 0.7927, Train Acc: 0.6687, Prec: 0.6579, Rec: 0.6696, F1: 0.6572 | Val Loss: 1.3115, Val Acc: 0.4702, Prec: 0.3175, Rec: 0.3772, F1: 0.3260\nEpoch 27/500: Train Loss: 0.7966, Train Acc: 0.6719, Prec: 0.6594, Rec: 0.6729, F1: 0.6599 | Val Loss: 1.4059, Val Acc: 0.3930, Prec: 0.3095, Rec: 0.3504, F1: 0.2964\nEpoch 28/500: Train Loss: 0.7633, Train Acc: 0.6815, Prec: 0.6690, Rec: 0.6825, F1: 0.6707 | Val Loss: 1.2633, Val Acc: 0.5263, Prec: 0.3337, Rec: 0.3698, F1: 0.3299\nEpoch 29/500: Train Loss: 0.7646, Train Acc: 0.6717, Prec: 0.6593, Rec: 0.6725, F1: 0.6606 | Val Loss: 1.3329, Val Acc: 0.4702, Prec: 0.3100, Rec: 0.3395, F1: 0.3117\nEpoch 30/500: Train Loss: 0.7425, Train Acc: 0.6881, Prec: 0.6751, Rec: 0.6891, F1: 0.6768 | Val Loss: 1.4023, Val Acc: 0.4386, Prec: 0.2875, Rec: 0.3151, F1: 0.2781\nEpoch 31/500: Train Loss: 0.7362, Train Acc: 0.6893, Prec: 0.6776, Rec: 0.6903, F1: 0.6795 | Val Loss: 1.3849, Val Acc: 0.4211, Prec: 0.2927, Rec: 0.3280, F1: 0.2828\nEpoch 32/500: Train Loss: 0.7147, Train Acc: 0.7079, Prec: 0.6978, Rec: 0.7091, F1: 0.6990 | Val Loss: 1.3259, Val Acc: 0.4737, Prec: 0.3100, Rec: 0.3544, F1: 0.3163\nEpoch 33/500: Train Loss: 0.7012, Train Acc: 0.7147, Prec: 0.7055, Rec: 0.7160, F1: 0.7061 | Val Loss: 1.3544, Val Acc: 0.4807, Prec: 0.3121, Rec: 0.3701, F1: 0.3230\nEpoch 34/500: Train Loss: 0.6978, Train Acc: 0.7157, Prec: 0.7069, Rec: 0.7169, F1: 0.7071 | Val Loss: 1.4286, Val Acc: 0.4140, Prec: 0.3042, Rec: 0.3390, F1: 0.2974\nEpoch 35/500: Train Loss: 0.6834, Train Acc: 0.7223, Prec: 0.7142, Rec: 0.7235, F1: 0.7157 | Val Loss: 1.3980, Val Acc: 0.4877, Prec: 0.3199, Rec: 0.3581, F1: 0.3210\nEpoch 36/500: Train Loss: 0.6730, Train Acc: 0.7259, Prec: 0.7182, Rec: 0.7272, F1: 0.7182 | Val Loss: 1.3949, Val Acc: 0.4596, Prec: 0.2890, Rec: 0.3151, F1: 0.2903\nEpoch 37/500: Train Loss: 0.6706, Train Acc: 0.7291, Prec: 0.7204, Rec: 0.7306, F1: 0.7218 | Val Loss: 1.3950, Val Acc: 0.4912, Prec: 0.3195, Rec: 0.3810, F1: 0.3222\nEpoch 38/500: Train Loss: 0.6505, Train Acc: 0.7361, Prec: 0.7274, Rec: 0.7376, F1: 0.7286 | Val Loss: 1.4060, Val Acc: 0.4632, Prec: 0.2982, Rec: 0.3235, F1: 0.2997\nEpoch 39/500: Train Loss: 0.6347, Train Acc: 0.7411, Prec: 0.7327, Rec: 0.7425, F1: 0.7335 | Val Loss: 1.4014, Val Acc: 0.4421, Prec: 0.3014, Rec: 0.3352, F1: 0.2960\nEpoch 40/500: Train Loss: 0.6247, Train Acc: 0.7439, Prec: 0.7359, Rec: 0.7456, F1: 0.7371 | Val Loss: 1.4535, Val Acc: 0.4281, Prec: 0.2887, Rec: 0.2946, F1: 0.2690\nEpoch 41/500: Train Loss: 0.6244, Train Acc: 0.7499, Prec: 0.7430, Rec: 0.7513, F1: 0.7437 | Val Loss: 1.4136, Val Acc: 0.4737, Prec: 0.3059, Rec: 0.3368, F1: 0.3088\nEpoch 42/500: Train Loss: 0.6135, Train Acc: 0.7521, Prec: 0.7460, Rec: 0.7537, F1: 0.7467 | Val Loss: 1.3506, Val Acc: 0.5053, Prec: 0.3026, Rec: 0.3315, F1: 0.3052\nEpoch 43/500: Train Loss: 0.5942, Train Acc: 0.7647, Prec: 0.7581, Rec: 0.7662, F1: 0.7588 | Val Loss: 1.4555, Val Acc: 0.4877, Prec: 0.3226, Rec: 0.3591, F1: 0.3241\nEpoch 44/500: Train Loss: 0.5832, Train Acc: 0.7659, Prec: 0.7591, Rec: 0.7676, F1: 0.7605 | Val Loss: 1.4460, Val Acc: 0.4667, Prec: 0.3027, Rec: 0.3277, F1: 0.3049\nEpoch 45/500: Train Loss: 0.5858, Train Acc: 0.7701, Prec: 0.7645, Rec: 0.7717, F1: 0.7650 | Val Loss: 1.4713, Val Acc: 0.4912, Prec: 0.3158, Rec: 0.3519, F1: 0.3190\nEpoch 46/500: Train Loss: 0.5675, Train Acc: 0.7705, Prec: 0.7645, Rec: 0.7722, F1: 0.7652 | Val Loss: 1.4185, Val Acc: 0.4772, Prec: 0.3088, Rec: 0.3417, F1: 0.3139\nEpoch 47/500: Train Loss: 0.5598, Train Acc: 0.7775, Prec: 0.7724, Rec: 0.7792, F1: 0.7729 | Val Loss: 1.4333, Val Acc: 0.4947, Prec: 0.3162, Rec: 0.3468, F1: 0.3213\nEpoch 48/500: Train Loss: 0.5615, Train Acc: 0.7699, Prec: 0.7634, Rec: 0.7715, F1: 0.7646 | Val Loss: 1.5097, Val Acc: 0.4667, Prec: 0.3021, Rec: 0.3201, F1: 0.2956\nEpoch 49/500: Train Loss: 0.5374, Train Acc: 0.7885, Prec: 0.7843, Rec: 0.7899, F1: 0.7851 | Val Loss: 1.4984, Val Acc: 0.4947, Prec: 0.3023, Rec: 0.3285, F1: 0.3063\nEpoch 50/500: Train Loss: 0.5460, Train Acc: 0.7853, Prec: 0.7808, Rec: 0.7869, F1: 0.7812 | Val Loss: 1.4645, Val Acc: 0.4842, Prec: 0.3014, Rec: 0.3266, F1: 0.3035\nEpoch 51/500: Train Loss: 0.5299, Train Acc: 0.7877, Prec: 0.7829, Rec: 0.7894, F1: 0.7832 | Val Loss: 1.4981, Val Acc: 0.4561, Prec: 0.2936, Rec: 0.3172, F1: 0.2935\nEpoch 52/500: Train Loss: 0.5230, Train Acc: 0.7907, Prec: 0.7854, Rec: 0.7926, F1: 0.7864 | Val Loss: 1.4755, Val Acc: 0.4807, Prec: 0.3056, Rec: 0.3356, F1: 0.3108\nEpoch 53/500: Train Loss: 0.5182, Train Acc: 0.8031, Prec: 0.7986, Rec: 0.8048, F1: 0.7990 | Val Loss: 1.5769, Val Acc: 0.4281, Prec: 0.2970, Rec: 0.3260, F1: 0.2915\nEpoch 54/500: Train Loss: 0.5001, Train Acc: 0.8029, Prec: 0.7989, Rec: 0.8046, F1: 0.7997 | Val Loss: 1.5676, Val Acc: 0.4491, Prec: 0.2970, Rec: 0.3291, F1: 0.2970\nEpoch 55/500: Train Loss: 0.5037, Train Acc: 0.8039, Prec: 0.8006, Rec: 0.8056, F1: 0.8004 | Val Loss: 1.5785, Val Acc: 0.4526, Prec: 0.2881, Rec: 0.3067, F1: 0.2818\nEpoch 56/500: Train Loss: 0.4794, Train Acc: 0.8111, Prec: 0.8073, Rec: 0.8127, F1: 0.8078 | Val Loss: 1.5581, Val Acc: 0.4842, Prec: 0.3008, Rec: 0.3262, F1: 0.3044\nEpoch 57/500: Train Loss: 0.4836, Train Acc: 0.8145, Prec: 0.8114, Rec: 0.8160, F1: 0.8114 | Val Loss: 1.6106, Val Acc: 0.4491, Prec: 0.3052, Rec: 0.3347, F1: 0.2984\nEpoch 58/500: Train Loss: 0.4781, Train Acc: 0.8095, Prec: 0.8063, Rec: 0.8111, F1: 0.8069 | Val Loss: 1.6072, Val Acc: 0.4702, Prec: 0.2977, Rec: 0.3260, F1: 0.2997\nEpoch 59/500: Train Loss: 0.4690, Train Acc: 0.8197, Prec: 0.8167, Rec: 0.8215, F1: 0.8168 | Val Loss: 1.5339, Val Acc: 0.5474, Prec: 0.3246, Rec: 0.3497, F1: 0.3299\nEpoch 60/500: Train Loss: 0.4616, Train Acc: 0.8149, Prec: 0.8118, Rec: 0.8166, F1: 0.8120 | Val Loss: 1.5939, Val Acc: 0.5053, Prec: 0.3033, Rec: 0.3245, F1: 0.3027\nEpoch 61/500: Train Loss: 0.4500, Train Acc: 0.8199, Prec: 0.8167, Rec: 0.8216, F1: 0.8169 | Val Loss: 1.5641, Val Acc: 0.5018, Prec: 0.3024, Rec: 0.3207, F1: 0.3047\nEpoch 62/500: Train Loss: 0.4424, Train Acc: 0.8267, Prec: 0.8238, Rec: 0.8283, F1: 0.8242 | Val Loss: 1.5887, Val Acc: 0.5053, Prec: 0.3028, Rec: 0.3183, F1: 0.3037\nEpoch 63/500: Train Loss: 0.4393, Train Acc: 0.8301, Prec: 0.8273, Rec: 0.8318, F1: 0.8277 | Val Loss: 1.6279, Val Acc: 0.4982, Prec: 0.3178, Rec: 0.3472, F1: 0.3219\nEpoch 64/500: Train Loss: 0.4278, Train Acc: 0.8329, Prec: 0.8303, Rec: 0.8344, F1: 0.8308 | Val Loss: 1.6203, Val Acc: 0.5368, Prec: 0.3253, Rec: 0.3454, F1: 0.3289\nEpoch 65/500: Train Loss: 0.4306, Train Acc: 0.8337, Prec: 0.8306, Rec: 0.8354, F1: 0.8313 | Val Loss: 1.6199, Val Acc: 0.5158, Prec: 0.3092, Rec: 0.3313, F1: 0.3106\nEpoch 66/500: Train Loss: 0.4104, Train Acc: 0.8479, Prec: 0.8461, Rec: 0.8496, F1: 0.8457 | Val Loss: 1.6153, Val Acc: 0.5333, Prec: 0.3023, Rec: 0.3157, F1: 0.3004\nEpoch 67/500: Train Loss: 0.4155, Train Acc: 0.8395, Prec: 0.8372, Rec: 0.8412, F1: 0.8376 | Val Loss: 1.6833, Val Acc: 0.4772, Prec: 0.2954, Rec: 0.3168, F1: 0.2968\nEpoch 68/500: Train Loss: 0.4101, Train Acc: 0.8429, Prec: 0.8411, Rec: 0.8446, F1: 0.8407 | Val Loss: 1.6499, Val Acc: 0.5018, Prec: 0.3028, Rec: 0.3172, F1: 0.3027\nEpoch 69/500: Train Loss: 0.4071, Train Acc: 0.8473, Prec: 0.8454, Rec: 0.8489, F1: 0.8453 | Val Loss: 1.6798, Val Acc: 0.5439, Prec: 0.3244, Rec: 0.3400, F1: 0.3250\nEpoch 70/500: Train Loss: 0.3972, Train Acc: 0.8485, Prec: 0.8463, Rec: 0.8501, F1: 0.8465 | Val Loss: 1.6606, Val Acc: 0.5649, Prec: 0.3106, Rec: 0.3120, F1: 0.3081\nEpoch 71/500: Train Loss: 0.3830, Train Acc: 0.8561, Prec: 0.8542, Rec: 0.8576, F1: 0.8546 | Val Loss: 1.7624, Val Acc: 0.5053, Prec: 0.3113, Rec: 0.3369, F1: 0.3139\nEpoch 72/500: Train Loss: 0.3827, Train Acc: 0.8537, Prec: 0.8520, Rec: 0.8554, F1: 0.8520 | Val Loss: 1.7764, Val Acc: 0.5053, Prec: 0.3137, Rec: 0.3283, F1: 0.3103\nEpoch 73/500: Train Loss: 0.3828, Train Acc: 0.8535, Prec: 0.8513, Rec: 0.8551, F1: 0.8518 | Val Loss: 1.7211, Val Acc: 0.5579, Prec: 0.3386, Rec: 0.3593, F1: 0.3453\nBest model saved with F1: 0.3453 at epoch 73\nEpoch 74/500: Train Loss: 0.3729, Train Acc: 0.8593, Prec: 0.8575, Rec: 0.8609, F1: 0.8579 | Val Loss: 1.7756, Val Acc: 0.5123, Prec: 0.3316, Rec: 0.3599, F1: 0.3355\nEpoch 75/500: Train Loss: 0.3686, Train Acc: 0.8621, Prec: 0.8610, Rec: 0.8636, F1: 0.8606 | Val Loss: 1.7557, Val Acc: 0.5474, Prec: 0.3333, Rec: 0.3598, F1: 0.3418\nEpoch 76/500: Train Loss: 0.3570, Train Acc: 0.8625, Prec: 0.8617, Rec: 0.8639, F1: 0.8612 | Val Loss: 1.6981, Val Acc: 0.5649, Prec: 0.3310, Rec: 0.3407, F1: 0.3312\nEpoch 77/500: Train Loss: 0.3657, Train Acc: 0.8601, Prec: 0.8584, Rec: 0.8615, F1: 0.8587 | Val Loss: 1.7185, Val Acc: 0.5719, Prec: 0.3392, Rec: 0.3530, F1: 0.3398\nEpoch 78/500: Train Loss: 0.3455, Train Acc: 0.8735, Prec: 0.8723, Rec: 0.8750, F1: 0.8724 | Val Loss: 1.7624, Val Acc: 0.5579, Prec: 0.3234, Rec: 0.3344, F1: 0.3251\nEpoch 79/500: Train Loss: 0.3523, Train Acc: 0.8709, Prec: 0.8702, Rec: 0.8724, F1: 0.8699 | Val Loss: 1.8072, Val Acc: 0.5298, Prec: 0.3005, Rec: 0.3249, F1: 0.3064\nEpoch 80/500: Train Loss: 0.3466, Train Acc: 0.8743, Prec: 0.8728, Rec: 0.8759, F1: 0.8730 | Val Loss: 1.8247, Val Acc: 0.5263, Prec: 0.3183, Rec: 0.3211, F1: 0.3109\nEpoch 81/500: Train Loss: 0.3399, Train Acc: 0.8743, Prec: 0.8739, Rec: 0.8758, F1: 0.8732 | Val Loss: 1.7913, Val Acc: 0.5509, Prec: 0.3229, Rec: 0.3322, F1: 0.3239\nEpoch 82/500: Train Loss: 0.3429, Train Acc: 0.8697, Prec: 0.8685, Rec: 0.8713, F1: 0.8683 | Val Loss: 1.8913, Val Acc: 0.4877, Prec: 0.3158, Rec: 0.3264, F1: 0.3085\nEpoch 83/500: Train Loss: 0.3284, Train Acc: 0.8780, Prec: 0.8776, Rec: 0.8796, F1: 0.8768 | Val Loss: 1.7983, Val Acc: 0.5544, Prec: 0.3216, Rec: 0.3236, F1: 0.3182\nEpoch 84/500: Train Loss: 0.3281, Train Acc: 0.8774, Prec: 0.8765, Rec: 0.8788, F1: 0.8766 | Val Loss: 1.8443, Val Acc: 0.5439, Prec: 0.3076, Rec: 0.3200, F1: 0.3109\nEpoch 85/500: Train Loss: 0.3202, Train Acc: 0.8792, Prec: 0.8786, Rec: 0.8807, F1: 0.8781 | Val Loss: 1.8836, Val Acc: 0.4912, Prec: 0.2928, Rec: 0.3064, F1: 0.2919\nEpoch 86/500: Train Loss: 0.3241, Train Acc: 0.8772, Prec: 0.8769, Rec: 0.8787, F1: 0.8763 | Val Loss: 1.8841, Val Acc: 0.5193, Prec: 0.2991, Rec: 0.3217, F1: 0.3037\nEpoch 87/500: Train Loss: 0.3159, Train Acc: 0.8828, Prec: 0.8822, Rec: 0.8842, F1: 0.8820 | Val Loss: 1.8797, Val Acc: 0.5509, Prec: 0.3218, Rec: 0.3328, F1: 0.3231\nEpoch 88/500: Train Loss: 0.3212, Train Acc: 0.8818, Prec: 0.8816, Rec: 0.8833, F1: 0.8808 | Val Loss: 1.8542, Val Acc: 0.5684, Prec: 0.3086, Rec: 0.3197, F1: 0.3094\nEpoch 89/500: Train Loss: 0.3072, Train Acc: 0.8872, Prec: 0.8862, Rec: 0.8887, F1: 0.8863 | Val Loss: 1.8752, Val Acc: 0.5509, Prec: 0.3198, Rec: 0.3318, F1: 0.3195\nEpoch 90/500: Train Loss: 0.3006, Train Acc: 0.8892, Prec: 0.8887, Rec: 0.8908, F1: 0.8883 | Val Loss: 1.8548, Val Acc: 0.6000, Prec: 0.3376, Rec: 0.3340, F1: 0.3346\nEpoch 91/500: Train Loss: 0.3031, Train Acc: 0.8840, Prec: 0.8835, Rec: 0.8854, F1: 0.8831 | Val Loss: 1.9161, Val Acc: 0.5333, Prec: 0.3411, Rec: 0.3516, F1: 0.3424\nEpoch 92/500: Train Loss: 0.2973, Train Acc: 0.8874, Prec: 0.8870, Rec: 0.8888, F1: 0.8868 | Val Loss: 1.8843, Val Acc: 0.5544, Prec: 0.3267, Rec: 0.3402, F1: 0.3314\nEpoch 93/500: Train Loss: 0.2906, Train Acc: 0.8882, Prec: 0.8880, Rec: 0.8897, F1: 0.8874 | Val Loss: 1.8856, Val Acc: 0.5930, Prec: 0.3295, Rec: 0.3280, F1: 0.3273\nEpoch 94/500: Train Loss: 0.2839, Train Acc: 0.8966, Prec: 0.8965, Rec: 0.8979, F1: 0.8959 | Val Loss: 1.9703, Val Acc: 0.5404, Prec: 0.3075, Rec: 0.3185, F1: 0.3111\nEpoch 95/500: Train Loss: 0.2884, Train Acc: 0.8906, Prec: 0.8904, Rec: 0.8920, F1: 0.8898 | Val Loss: 2.0077, Val Acc: 0.5333, Prec: 0.3120, Rec: 0.3233, F1: 0.3135\nEpoch 96/500: Train Loss: 0.2751, Train Acc: 0.9000, Prec: 0.8995, Rec: 0.9013, F1: 0.8995 | Val Loss: 2.0215, Val Acc: 0.5684, Prec: 0.3244, Rec: 0.3204, F1: 0.3191\nEpoch 97/500: Train Loss: 0.2778, Train Acc: 0.8946, Prec: 0.8944, Rec: 0.8959, F1: 0.8941 | Val Loss: 1.9941, Val Acc: 0.5895, Prec: 0.3182, Rec: 0.3059, F1: 0.3088\nEpoch 98/500: Train Loss: 0.2831, Train Acc: 0.9014, Prec: 0.9016, Rec: 0.9028, F1: 0.9008 | Val Loss: 1.9764, Val Acc: 0.5614, Prec: 0.3055, Rec: 0.3082, F1: 0.3055\nEpoch 99/500: Train Loss: 0.2777, Train Acc: 0.8996, Prec: 0.8995, Rec: 0.9010, F1: 0.8990 | Val Loss: 1.9986, Val Acc: 0.5789, Prec: 0.3429, Rec: 0.3519, F1: 0.3464\nBest model saved with F1: 0.3464 at epoch 99\nEpoch 100/500: Train Loss: 0.2721, Train Acc: 0.8996, Prec: 0.8995, Rec: 0.9009, F1: 0.8992 | Val Loss: 2.0565, Val Acc: 0.5404, Prec: 0.3165, Rec: 0.3289, F1: 0.3175\nEpoch 101/500: Train Loss: 0.2606, Train Acc: 0.9060, Prec: 0.9058, Rec: 0.9072, F1: 0.9056 | Val Loss: 2.0879, Val Acc: 0.5930, Prec: 0.3172, Rec: 0.3374, F1: 0.3215\nEpoch 102/500: Train Loss: 0.2499, Train Acc: 0.9082, Prec: 0.9080, Rec: 0.9095, F1: 0.9077 | Val Loss: 2.1235, Val Acc: 0.5754, Prec: 0.3134, Rec: 0.3153, F1: 0.3112\nEpoch 103/500: Train Loss: 0.2517, Train Acc: 0.9080, Prec: 0.9083, Rec: 0.9092, F1: 0.9077 | Val Loss: 2.0965, Val Acc: 0.5789, Prec: 0.3186, Rec: 0.3233, F1: 0.3192\nEpoch 104/500: Train Loss: 0.2590, Train Acc: 0.9034, Prec: 0.9032, Rec: 0.9046, F1: 0.9031 | Val Loss: 2.2415, Val Acc: 0.5439, Prec: 0.3121, Rec: 0.3030, F1: 0.3007\nEpoch 105/500: Train Loss: 0.2653, Train Acc: 0.9044, Prec: 0.9047, Rec: 0.9056, F1: 0.9041 | Val Loss: 2.1312, Val Acc: 0.5930, Prec: 0.3397, Rec: 0.3456, F1: 0.3413\nEpoch 106/500: Train Loss: 0.2482, Train Acc: 0.9134, Prec: 0.9140, Rec: 0.9146, F1: 0.9130 | Val Loss: 2.1211, Val Acc: 0.6035, Prec: 0.3442, Rec: 0.3417, F1: 0.3421\nEpoch 107/500: Train Loss: 0.2514, Train Acc: 0.9106, Prec: 0.9105, Rec: 0.9119, F1: 0.9102 | Val Loss: 2.1927, Val Acc: 0.5684, Prec: 0.3255, Rec: 0.3304, F1: 0.3246\nEpoch 108/500: Train Loss: 0.2410, Train Acc: 0.9174, Prec: 0.9179, Rec: 0.9186, F1: 0.9172 | Val Loss: 2.1500, Val Acc: 0.5965, Prec: 0.3513, Rec: 0.3470, F1: 0.3464\nEpoch 109/500: Train Loss: 0.2469, Train Acc: 0.9086, Prec: 0.9089, Rec: 0.9100, F1: 0.9082 | Val Loss: 2.1515, Val Acc: 0.5754, Prec: 0.3274, Rec: 0.3360, F1: 0.3289\nEpoch 110/500: Train Loss: 0.2411, Train Acc: 0.9148, Prec: 0.9148, Rec: 0.9160, F1: 0.9144 | Val Loss: 2.1327, Val Acc: 0.5649, Prec: 0.3160, Rec: 0.3155, F1: 0.3126\nEpoch 111/500: Train Loss: 0.2394, Train Acc: 0.9088, Prec: 0.9088, Rec: 0.9101, F1: 0.9086 | Val Loss: 2.2202, Val Acc: 0.5895, Prec: 0.3216, Rec: 0.3297, F1: 0.3220\nEpoch 112/500: Train Loss: 0.2384, Train Acc: 0.9116, Prec: 0.9120, Rec: 0.9129, F1: 0.9113 | Val Loss: 2.2351, Val Acc: 0.5404, Prec: 0.3216, Rec: 0.3399, F1: 0.3261\nEpoch 113/500: Train Loss: 0.2305, Train Acc: 0.9152, Prec: 0.9153, Rec: 0.9164, F1: 0.9149 | Val Loss: 2.2467, Val Acc: 0.5579, Prec: 0.3251, Rec: 0.3312, F1: 0.3251\nEpoch 114/500: Train Loss: 0.2270, Train Acc: 0.9188, Prec: 0.9192, Rec: 0.9200, F1: 0.9186 | Val Loss: 2.2218, Val Acc: 0.5719, Prec: 0.3365, Rec: 0.3322, F1: 0.3315\nEpoch 115/500: Train Loss: 0.2287, Train Acc: 0.9176, Prec: 0.9177, Rec: 0.9187, F1: 0.9173 | Val Loss: 2.2635, Val Acc: 0.5825, Prec: 0.3153, Rec: 0.3275, F1: 0.3187\nEpoch 116/500: Train Loss: 0.2337, Train Acc: 0.9174, Prec: 0.9174, Rec: 0.9186, F1: 0.9171 | Val Loss: 2.2160, Val Acc: 0.5930, Prec: 0.3346, Rec: 0.3415, F1: 0.3367\nEpoch 117/500: Train Loss: 0.2337, Train Acc: 0.9152, Prec: 0.9154, Rec: 0.9162, F1: 0.9151 | Val Loss: 2.1996, Val Acc: 0.6000, Prec: 0.3434, Rec: 0.3447, F1: 0.3398\nEpoch 118/500: Train Loss: 0.2175, Train Acc: 0.9198, Prec: 0.9201, Rec: 0.9209, F1: 0.9196 | Val Loss: 2.2631, Val Acc: 0.5614, Prec: 0.3156, Rec: 0.3185, F1: 0.3144\nEpoch 119/500: Train Loss: 0.2296, Train Acc: 0.9136, Prec: 0.9138, Rec: 0.9147, F1: 0.9134 | Val Loss: 2.2440, Val Acc: 0.5789, Prec: 0.3212, Rec: 0.3237, F1: 0.3210\nEpoch 120/500: Train Loss: 0.2147, Train Acc: 0.9244, Prec: 0.9245, Rec: 0.9256, F1: 0.9242 | Val Loss: 2.3722, Val Acc: 0.5719, Prec: 0.3289, Rec: 0.3377, F1: 0.3207\nEpoch 121/500: Train Loss: 0.2174, Train Acc: 0.9238, Prec: 0.9240, Rec: 0.9249, F1: 0.9236 | Val Loss: 2.3803, Val Acc: 0.5404, Prec: 0.3059, Rec: 0.3213, F1: 0.3085\nEpoch 122/500: Train Loss: 0.2216, Train Acc: 0.9192, Prec: 0.9195, Rec: 0.9203, F1: 0.9190 | Val Loss: 2.3426, Val Acc: 0.5684, Prec: 0.3365, Rec: 0.3376, F1: 0.3332\nEpoch 123/500: Train Loss: 0.2244, Train Acc: 0.9182, Prec: 0.9184, Rec: 0.9192, F1: 0.9182 | Val Loss: 2.4003, Val Acc: 0.5579, Prec: 0.3244, Rec: 0.3212, F1: 0.3182\nEpoch 124/500: Train Loss: 0.2193, Train Acc: 0.9230, Prec: 0.9231, Rec: 0.9241, F1: 0.9228 | Val Loss: 2.3380, Val Acc: 0.5544, Prec: 0.3259, Rec: 0.3301, F1: 0.3242\nEpoch 125/500: Train Loss: 0.2181, Train Acc: 0.9218, Prec: 0.9221, Rec: 0.9228, F1: 0.9216 | Val Loss: 2.3571, Val Acc: 0.5474, Prec: 0.3108, Rec: 0.3307, F1: 0.3158\nEpoch 126/500: Train Loss: 0.2129, Train Acc: 0.9218, Prec: 0.9219, Rec: 0.9229, F1: 0.9215 | Val Loss: 2.3421, Val Acc: 0.5754, Prec: 0.3270, Rec: 0.3360, F1: 0.3290\nEpoch 127/500: Train Loss: 0.2087, Train Acc: 0.9252, Prec: 0.9258, Rec: 0.9262, F1: 0.9250 | Val Loss: 2.3524, Val Acc: 0.5825, Prec: 0.3348, Rec: 0.3521, F1: 0.3381\nEpoch 128/500: Train Loss: 0.1992, Train Acc: 0.9298, Prec: 0.9299, Rec: 0.9307, F1: 0.9298 | Val Loss: 2.3546, Val Acc: 0.5719, Prec: 0.3182, Rec: 0.3280, F1: 0.3212\nEpoch 129/500: Train Loss: 0.2130, Train Acc: 0.9262, Prec: 0.9265, Rec: 0.9271, F1: 0.9260 | Val Loss: 2.3811, Val Acc: 0.5649, Prec: 0.3344, Rec: 0.3362, F1: 0.3290\nEpoch 130/500: Train Loss: 0.2014, Train Acc: 0.9270, Prec: 0.9274, Rec: 0.9280, F1: 0.9269 | Val Loss: 2.3510, Val Acc: 0.5895, Prec: 0.3368, Rec: 0.3269, F1: 0.3286\nEpoch 131/500: Train Loss: 0.1989, Train Acc: 0.9278, Prec: 0.9280, Rec: 0.9289, F1: 0.9276 | Val Loss: 2.4243, Val Acc: 0.5895, Prec: 0.3307, Rec: 0.3336, F1: 0.3306\nEpoch 132/500: Train Loss: 0.2030, Train Acc: 0.9284, Prec: 0.9285, Rec: 0.9292, F1: 0.9284 | Val Loss: 2.3946, Val Acc: 0.5789, Prec: 0.3315, Rec: 0.3302, F1: 0.3286\nEpoch 133/500: Train Loss: 0.1894, Train Acc: 0.9306, Prec: 0.9306, Rec: 0.9317, F1: 0.9305 | Val Loss: 2.4456, Val Acc: 0.5579, Prec: 0.3085, Rec: 0.3202, F1: 0.3121\nEpoch 134/500: Train Loss: 0.1900, Train Acc: 0.9272, Prec: 0.9276, Rec: 0.9283, F1: 0.9273 | Val Loss: 2.5671, Val Acc: 0.5684, Prec: 0.3243, Rec: 0.3235, F1: 0.3153\nEpoch 135/500: Train Loss: 0.2048, Train Acc: 0.9254, Prec: 0.9254, Rec: 0.9263, F1: 0.9252 | Val Loss: 2.5146, Val Acc: 0.5684, Prec: 0.3280, Rec: 0.3304, F1: 0.3262\nEpoch 136/500: Train Loss: 0.1886, Train Acc: 0.9316, Prec: 0.9319, Rec: 0.9326, F1: 0.9315 | Val Loss: 2.3740, Val Acc: 0.5895, Prec: 0.3254, Rec: 0.3332, F1: 0.3264\nEpoch 137/500: Train Loss: 0.1867, Train Acc: 0.9368, Prec: 0.9372, Rec: 0.9378, F1: 0.9367 | Val Loss: 2.4806, Val Acc: 0.5579, Prec: 0.3198, Rec: 0.3209, F1: 0.3141\nEpoch 138/500: Train Loss: 0.2027, Train Acc: 0.9308, Prec: 0.9309, Rec: 0.9318, F1: 0.9307 | Val Loss: 2.4911, Val Acc: 0.5825, Prec: 0.3217, Rec: 0.3279, F1: 0.3241\nEpoch 139/500: Train Loss: 0.1859, Train Acc: 0.9370, Prec: 0.9372, Rec: 0.9380, F1: 0.9370 | Val Loss: 2.4758, Val Acc: 0.5579, Prec: 0.3027, Rec: 0.3095, F1: 0.3056\nEpoch 140/500: Train Loss: 0.1870, Train Acc: 0.9362, Prec: 0.9364, Rec: 0.9371, F1: 0.9362 | Val Loss: 2.5195, Val Acc: 0.5895, Prec: 0.3531, Rec: 0.3480, F1: 0.3479\nBest model saved with F1: 0.3479 at epoch 140\nEpoch 141/500: Train Loss: 0.1940, Train Acc: 0.9322, Prec: 0.9326, Rec: 0.9331, F1: 0.9322 | Val Loss: 2.4446, Val Acc: 0.5860, Prec: 0.3229, Rec: 0.3286, F1: 0.3230\nEpoch 142/500: Train Loss: 0.1807, Train Acc: 0.9342, Prec: 0.9342, Rec: 0.9352, F1: 0.9342 | Val Loss: 2.5436, Val Acc: 0.5965, Prec: 0.3339, Rec: 0.3391, F1: 0.3360\nEpoch 143/500: Train Loss: 0.1765, Train Acc: 0.9374, Prec: 0.9379, Rec: 0.9384, F1: 0.9373 | Val Loss: 2.5658, Val Acc: 0.5754, Prec: 0.3138, Rec: 0.3157, F1: 0.3139\nEpoch 144/500: Train Loss: 0.1830, Train Acc: 0.9346, Prec: 0.9348, Rec: 0.9355, F1: 0.9346 | Val Loss: 2.6015, Val Acc: 0.5649, Prec: 0.3397, Rec: 0.3510, F1: 0.3408\nEpoch 145/500: Train Loss: 0.1743, Train Acc: 0.9400, Prec: 0.9403, Rec: 0.9408, F1: 0.9399 | Val Loss: 2.6534, Val Acc: 0.5754, Prec: 0.3330, Rec: 0.3398, F1: 0.3347\nEpoch 146/500: Train Loss: 0.1870, Train Acc: 0.9318, Prec: 0.9321, Rec: 0.9327, F1: 0.9318 | Val Loss: 2.5263, Val Acc: 0.5895, Prec: 0.3294, Rec: 0.3332, F1: 0.3280\nEpoch 147/500: Train Loss: 0.1808, Train Acc: 0.9334, Prec: 0.9337, Rec: 0.9344, F1: 0.9334 | Val Loss: 2.5081, Val Acc: 0.5965, Prec: 0.3373, Rec: 0.3291, F1: 0.3322\nEpoch 148/500: Train Loss: 0.1761, Train Acc: 0.9376, Prec: 0.9377, Rec: 0.9385, F1: 0.9375 | Val Loss: 2.6215, Val Acc: 0.6070, Prec: 0.3555, Rec: 0.3431, F1: 0.3480\nBest model saved with F1: 0.3480 at epoch 148\nEpoch 149/500: Train Loss: 0.1793, Train Acc: 0.9370, Prec: 0.9372, Rec: 0.9379, F1: 0.9369 | Val Loss: 2.5957, Val Acc: 0.5754, Prec: 0.3306, Rec: 0.3360, F1: 0.3301\nEpoch 150/500: Train Loss: 0.1732, Train Acc: 0.9372, Prec: 0.9374, Rec: 0.9381, F1: 0.9372 | Val Loss: 2.6349, Val Acc: 0.6105, Prec: 0.3288, Rec: 0.3259, F1: 0.3232\nEpoch 151/500: Train Loss: 0.1717, Train Acc: 0.9406, Prec: 0.9409, Rec: 0.9415, F1: 0.9406 | Val Loss: 2.5877, Val Acc: 0.5789, Prec: 0.3261, Rec: 0.3333, F1: 0.3245\nEpoch 152/500: Train Loss: 0.1729, Train Acc: 0.9370, Prec: 0.9370, Rec: 0.9378, F1: 0.9370 | Val Loss: 2.6377, Val Acc: 0.5684, Prec: 0.3091, Rec: 0.3097, F1: 0.3084\nEpoch 153/500: Train Loss: 0.1669, Train Acc: 0.9402, Prec: 0.9405, Rec: 0.9411, F1: 0.9401 | Val Loss: 2.7635, Val Acc: 0.5754, Prec: 0.3355, Rec: 0.3291, F1: 0.3284\nEpoch 154/500: Train Loss: 0.1621, Train Acc: 0.9428, Prec: 0.9430, Rec: 0.9437, F1: 0.9428 | Val Loss: 2.6901, Val Acc: 0.6000, Prec: 0.3530, Rec: 0.3440, F1: 0.3458\nEpoch 155/500: Train Loss: 0.1645, Train Acc: 0.9392, Prec: 0.9394, Rec: 0.9401, F1: 0.9392 | Val Loss: 2.6641, Val Acc: 0.6070, Prec: 0.3613, Rec: 0.3534, F1: 0.3555\nBest model saved with F1: 0.3555 at epoch 155\nEpoch 156/500: Train Loss: 0.1725, Train Acc: 0.9388, Prec: 0.9391, Rec: 0.9397, F1: 0.9387 | Val Loss: 2.7573, Val Acc: 0.6105, Prec: 0.3386, Rec: 0.3225, F1: 0.3239\nEpoch 157/500: Train Loss: 0.1618, Train Acc: 0.9416, Prec: 0.9417, Rec: 0.9425, F1: 0.9416 | Val Loss: 2.7196, Val Acc: 0.5404, Prec: 0.3145, Rec: 0.3179, F1: 0.3093\nEpoch 158/500: Train Loss: 0.1608, Train Acc: 0.9434, Prec: 0.9438, Rec: 0.9442, F1: 0.9434 | Val Loss: 2.7192, Val Acc: 0.5930, Prec: 0.3443, Rec: 0.3349, F1: 0.3347\nEpoch 159/500: Train Loss: 0.1634, Train Acc: 0.9384, Prec: 0.9387, Rec: 0.9393, F1: 0.9384 | Val Loss: 2.7243, Val Acc: 0.5544, Prec: 0.3086, Rec: 0.3188, F1: 0.3096\nEpoch 160/500: Train Loss: 0.1626, Train Acc: 0.9428, Prec: 0.9427, Rec: 0.9436, F1: 0.9428 | Val Loss: 2.7289, Val Acc: 0.5930, Prec: 0.3575, Rec: 0.3491, F1: 0.3486\nEpoch 161/500: Train Loss: 0.1812, Train Acc: 0.9330, Prec: 0.9332, Rec: 0.9339, F1: 0.9331 | Val Loss: 2.6198, Val Acc: 0.5860, Prec: 0.3171, Rec: 0.3217, F1: 0.3168\nEpoch 162/500: Train Loss: 0.1504, Train Acc: 0.9452, Prec: 0.9454, Rec: 0.9460, F1: 0.9453 | Val Loss: 2.8324, Val Acc: 0.5719, Prec: 0.3169, Rec: 0.3211, F1: 0.3185\nEpoch 163/500: Train Loss: 0.1605, Train Acc: 0.9408, Prec: 0.9409, Rec: 0.9416, F1: 0.9409 | Val Loss: 2.8631, Val Acc: 0.5579, Prec: 0.3453, Rec: 0.3312, F1: 0.3293\nEpoch 164/500: Train Loss: 0.1602, Train Acc: 0.9434, Prec: 0.9436, Rec: 0.9441, F1: 0.9434 | Val Loss: 2.9206, Val Acc: 0.5333, Prec: 0.3230, Rec: 0.3270, F1: 0.3186\nEpoch 165/500: Train Loss: 0.1526, Train Acc: 0.9456, Prec: 0.9458, Rec: 0.9463, F1: 0.9456 | Val Loss: 2.8156, Val Acc: 0.6035, Prec: 0.3363, Rec: 0.3241, F1: 0.3291\nEpoch 166/500: Train Loss: 0.1512, Train Acc: 0.9440, Prec: 0.9440, Rec: 0.9447, F1: 0.9441 | Val Loss: 2.7268, Val Acc: 0.6070, Prec: 0.3364, Rec: 0.3317, F1: 0.3314\nEpoch 167/500: Train Loss: 0.1609, Train Acc: 0.9404, Prec: 0.9404, Rec: 0.9413, F1: 0.9404 | Val Loss: 2.7827, Val Acc: 0.6070, Prec: 0.3442, Rec: 0.3490, F1: 0.3401\nEpoch 168/500: Train Loss: 0.1575, Train Acc: 0.9426, Prec: 0.9428, Rec: 0.9433, F1: 0.9428 | Val Loss: 2.8009, Val Acc: 0.5860, Prec: 0.3459, Rec: 0.3431, F1: 0.3413\nEpoch 169/500: Train Loss: 0.1489, Train Acc: 0.9486, Prec: 0.9488, Rec: 0.9493, F1: 0.9486 | Val Loss: 2.8422, Val Acc: 0.5228, Prec: 0.3198, Rec: 0.3341, F1: 0.3207\nEpoch 170/500: Train Loss: 0.1556, Train Acc: 0.9470, Prec: 0.9473, Rec: 0.9477, F1: 0.9472 | Val Loss: 2.8043, Val Acc: 0.5614, Prec: 0.3215, Rec: 0.3317, F1: 0.3219\nEpoch 171/500: Train Loss: 0.1463, Train Acc: 0.9492, Prec: 0.9497, Rec: 0.9499, F1: 0.9493 | Val Loss: 2.9033, Val Acc: 0.5825, Prec: 0.3239, Rec: 0.3216, F1: 0.3219\nEpoch 172/500: Train Loss: 0.1602, Train Acc: 0.9410, Prec: 0.9413, Rec: 0.9418, F1: 0.9411 | Val Loss: 2.8108, Val Acc: 0.5719, Prec: 0.3029, Rec: 0.3136, F1: 0.3048\nEpoch 173/500: Train Loss: 0.1494, Train Acc: 0.9466, Prec: 0.9470, Rec: 0.9474, F1: 0.9467 | Val Loss: 2.9147, Val Acc: 0.6070, Prec: 0.3670, Rec: 0.3645, F1: 0.3651\nBest model saved with F1: 0.3651 at epoch 173\nEpoch 174/500: Train Loss: 0.1510, Train Acc: 0.9458, Prec: 0.9461, Rec: 0.9465, F1: 0.9459 | Val Loss: 2.8409, Val Acc: 0.5754, Prec: 0.3302, Rec: 0.3329, F1: 0.3305\nEpoch 175/500: Train Loss: 0.1570, Train Acc: 0.9436, Prec: 0.9438, Rec: 0.9443, F1: 0.9436 | Val Loss: 2.8334, Val Acc: 0.6070, Prec: 0.3353, Rec: 0.3217, F1: 0.3248\nEpoch 176/500: Train Loss: 0.1499, Train Acc: 0.9430, Prec: 0.9432, Rec: 0.9438, F1: 0.9429 | Val Loss: 2.8743, Val Acc: 0.5930, Prec: 0.3181, Rec: 0.3170, F1: 0.3156\nEpoch 177/500: Train Loss: 0.1448, Train Acc: 0.9466, Prec: 0.9469, Rec: 0.9473, F1: 0.9467 | Val Loss: 2.9281, Val Acc: 0.5474, Prec: 0.3150, Rec: 0.3173, F1: 0.3137\nEpoch 178/500: Train Loss: 0.1506, Train Acc: 0.9476, Prec: 0.9478, Rec: 0.9483, F1: 0.9476 | Val Loss: 2.8220, Val Acc: 0.5825, Prec: 0.3365, Rec: 0.3289, F1: 0.3303\nEpoch 179/500: Train Loss: 0.1469, Train Acc: 0.9480, Prec: 0.9481, Rec: 0.9487, F1: 0.9480 | Val Loss: 2.9164, Val Acc: 0.6000, Prec: 0.3338, Rec: 0.3192, F1: 0.3229\nEpoch 180/500: Train Loss: 0.1355, Train Acc: 0.9546, Prec: 0.9547, Rec: 0.9553, F1: 0.9547 | Val Loss: 2.8669, Val Acc: 0.5930, Prec: 0.3223, Rec: 0.3274, F1: 0.3204\nEpoch 181/500: Train Loss: 0.1382, Train Acc: 0.9506, Prec: 0.9508, Rec: 0.9513, F1: 0.9507 | Val Loss: 2.9699, Val Acc: 0.6070, Prec: 0.3545, Rec: 0.3493, F1: 0.3489\nEpoch 182/500: Train Loss: 0.1385, Train Acc: 0.9472, Prec: 0.9472, Rec: 0.9479, F1: 0.9472 | Val Loss: 2.9664, Val Acc: 0.5789, Prec: 0.3276, Rec: 0.3306, F1: 0.3276\nEpoch 183/500: Train Loss: 0.1450, Train Acc: 0.9484, Prec: 0.9486, Rec: 0.9490, F1: 0.9485 | Val Loss: 3.0213, Val Acc: 0.5684, Prec: 0.3203, Rec: 0.3266, F1: 0.3198\nEpoch 184/500: Train Loss: 0.1367, Train Acc: 0.9504, Prec: 0.9505, Rec: 0.9512, F1: 0.9504 | Val Loss: 3.0960, Val Acc: 0.6070, Prec: 0.3581, Rec: 0.3424, F1: 0.3425\nEpoch 185/500: Train Loss: 0.1269, Train Acc: 0.9582, Prec: 0.9586, Rec: 0.9589, F1: 0.9583 | Val Loss: 2.9976, Val Acc: 0.5930, Prec: 0.3394, Rec: 0.3353, F1: 0.3356\nEpoch 186/500: Train Loss: 0.1410, Train Acc: 0.9500, Prec: 0.9502, Rec: 0.9507, F1: 0.9501 | Val Loss: 2.9897, Val Acc: 0.5860, Prec: 0.3187, Rec: 0.3217, F1: 0.3171\nEpoch 187/500: Train Loss: 0.1444, Train Acc: 0.9470, Prec: 0.9471, Rec: 0.9477, F1: 0.9470 | Val Loss: 2.8671, Val Acc: 0.5789, Prec: 0.3096, Rec: 0.3161, F1: 0.3119\nEpoch 188/500: Train Loss: 0.1275, Train Acc: 0.9556, Prec: 0.9560, Rec: 0.9563, F1: 0.9557 | Val Loss: 2.9622, Val Acc: 0.5754, Prec: 0.3391, Rec: 0.3364, F1: 0.3358\nEpoch 189/500: Train Loss: 0.1250, Train Acc: 0.9572, Prec: 0.9574, Rec: 0.9577, F1: 0.9574 | Val Loss: 3.0481, Val Acc: 0.5860, Prec: 0.3093, Rec: 0.3145, F1: 0.3100\nEpoch 190/500: Train Loss: 0.1359, Train Acc: 0.9536, Prec: 0.9540, Rec: 0.9543, F1: 0.9538 | Val Loss: 2.9967, Val Acc: 0.6140, Prec: 0.3404, Rec: 0.3374, F1: 0.3375\nEpoch 191/500: Train Loss: 0.1319, Train Acc: 0.9540, Prec: 0.9542, Rec: 0.9547, F1: 0.9541 | Val Loss: 3.1011, Val Acc: 0.6035, Prec: 0.3361, Rec: 0.3272, F1: 0.3242\nEpoch 192/500: Train Loss: 0.1438, Train Acc: 0.9468, Prec: 0.9470, Rec: 0.9475, F1: 0.9469 | Val Loss: 2.9995, Val Acc: 0.5649, Prec: 0.3100, Rec: 0.3221, F1: 0.3143\nEpoch 193/500: Train Loss: 0.1422, Train Acc: 0.9488, Prec: 0.9492, Rec: 0.9494, F1: 0.9488 | Val Loss: 2.9665, Val Acc: 0.5965, Prec: 0.3522, Rec: 0.3574, F1: 0.3539\nEpoch 194/500: Train Loss: 0.1298, Train Acc: 0.9534, Prec: 0.9535, Rec: 0.9540, F1: 0.9536 | Val Loss: 3.0646, Val Acc: 0.5684, Prec: 0.3382, Rec: 0.3414, F1: 0.3346\nEpoch 195/500: Train Loss: 0.1198, Train Acc: 0.9598, Prec: 0.9599, Rec: 0.9605, F1: 0.9599 | Val Loss: 3.0591, Val Acc: 0.5614, Prec: 0.3245, Rec: 0.3382, F1: 0.3252\nEpoch 196/500: Train Loss: 0.1347, Train Acc: 0.9526, Prec: 0.9527, Rec: 0.9533, F1: 0.9527 | Val Loss: 3.0997, Val Acc: 0.5965, Prec: 0.3345, Rec: 0.3253, F1: 0.3270\nEpoch 197/500: Train Loss: 0.1267, Train Acc: 0.9558, Prec: 0.9559, Rec: 0.9564, F1: 0.9559 | Val Loss: 3.0297, Val Acc: 0.5649, Prec: 0.3221, Rec: 0.3231, F1: 0.3202\nEpoch 198/500: Train Loss: 0.1341, Train Acc: 0.9512, Prec: 0.9514, Rec: 0.9518, F1: 0.9513 | Val Loss: 3.1442, Val Acc: 0.5684, Prec: 0.3112, Rec: 0.3204, F1: 0.3135\nEpoch 199/500: Train Loss: 0.1371, Train Acc: 0.9506, Prec: 0.9507, Rec: 0.9512, F1: 0.9507 | Val Loss: 3.0258, Val Acc: 0.5719, Prec: 0.3068, Rec: 0.3108, F1: 0.3084\nEpoch 200/500: Train Loss: 0.1420, Train Acc: 0.9500, Prec: 0.9501, Rec: 0.9506, F1: 0.9500 | Val Loss: 3.1204, Val Acc: 0.5965, Prec: 0.3313, Rec: 0.3388, F1: 0.3335\nEpoch 201/500: Train Loss: 0.1239, Train Acc: 0.9562, Prec: 0.9562, Rec: 0.9569, F1: 0.9563 | Val Loss: 3.0537, Val Acc: 0.5895, Prec: 0.3257, Rec: 0.3297, F1: 0.3235\nEpoch 202/500: Train Loss: 0.1294, Train Acc: 0.9524, Prec: 0.9526, Rec: 0.9531, F1: 0.9525 | Val Loss: 3.1188, Val Acc: 0.6105, Prec: 0.3501, Rec: 0.3476, F1: 0.3484\nEpoch 203/500: Train Loss: 0.1306, Train Acc: 0.9546, Prec: 0.9549, Rec: 0.9553, F1: 0.9547 | Val Loss: 3.0332, Val Acc: 0.6000, Prec: 0.3518, Rec: 0.3475, F1: 0.3488\nEpoch 204/500: Train Loss: 0.1200, Train Acc: 0.9566, Prec: 0.9567, Rec: 0.9573, F1: 0.9567 | Val Loss: 3.1536, Val Acc: 0.6140, Prec: 0.3396, Rec: 0.3374, F1: 0.3367\nEpoch 205/500: Train Loss: 0.1362, Train Acc: 0.9520, Prec: 0.9521, Rec: 0.9527, F1: 0.9520 | Val Loss: 3.0024, Val Acc: 0.6070, Prec: 0.3521, Rec: 0.3428, F1: 0.3462\nEpoch 206/500: Train Loss: 0.1363, Train Acc: 0.9500, Prec: 0.9500, Rec: 0.9505, F1: 0.9501 | Val Loss: 3.2141, Val Acc: 0.5825, Prec: 0.3306, Rec: 0.3323, F1: 0.3286\nEpoch 207/500: Train Loss: 0.1319, Train Acc: 0.9526, Prec: 0.9525, Rec: 0.9533, F1: 0.9527 | Val Loss: 3.2461, Val Acc: 0.5789, Prec: 0.3147, Rec: 0.3195, F1: 0.3134\nEpoch 208/500: Train Loss: 0.1196, Train Acc: 0.9574, Prec: 0.9576, Rec: 0.9581, F1: 0.9575 | Val Loss: 3.1100, Val Acc: 0.6140, Prec: 0.3467, Rec: 0.3377, F1: 0.3416\nEpoch 209/500: Train Loss: 0.1147, Train Acc: 0.9588, Prec: 0.9589, Rec: 0.9593, F1: 0.9588 | Val Loss: 3.3036, Val Acc: 0.5895, Prec: 0.3173, Rec: 0.3225, F1: 0.3082\nEpoch 210/500: Train Loss: 0.1216, Train Acc: 0.9546, Prec: 0.9547, Rec: 0.9552, F1: 0.9547 | Val Loss: 3.1871, Val Acc: 0.6070, Prec: 0.3370, Rec: 0.3386, F1: 0.3364\nEpoch 211/500: Train Loss: 0.1287, Train Acc: 0.9542, Prec: 0.9544, Rec: 0.9547, F1: 0.9543 | Val Loss: 3.2841, Val Acc: 0.6140, Prec: 0.3351, Rec: 0.3374, F1: 0.3341\nEpoch 212/500: Train Loss: 0.1225, Train Acc: 0.9548, Prec: 0.9550, Rec: 0.9556, F1: 0.9549 | Val Loss: 3.2779, Val Acc: 0.6175, Prec: 0.3300, Rec: 0.3177, F1: 0.3207\nEpoch 213/500: Train Loss: 0.1343, Train Acc: 0.9486, Prec: 0.9488, Rec: 0.9493, F1: 0.9486 | Val Loss: 3.0700, Val Acc: 0.6035, Prec: 0.3442, Rec: 0.3417, F1: 0.3414\nEpoch 214/500: Train Loss: 0.1220, Train Acc: 0.9584, Prec: 0.9586, Rec: 0.9590, F1: 0.9585 | Val Loss: 3.0842, Val Acc: 0.6035, Prec: 0.3617, Rec: 0.3630, F1: 0.3611\nEpoch 215/500: Train Loss: 0.1166, Train Acc: 0.9578, Prec: 0.9579, Rec: 0.9581, F1: 0.9578 | Val Loss: 3.1402, Val Acc: 0.6140, Prec: 0.3526, Rec: 0.3515, F1: 0.3498\nEpoch 216/500: Train Loss: 0.1245, Train Acc: 0.9548, Prec: 0.9550, Rec: 0.9554, F1: 0.9549 | Val Loss: 3.0299, Val Acc: 0.6140, Prec: 0.3530, Rec: 0.3546, F1: 0.3511\nEpoch 217/500: Train Loss: 0.1214, Train Acc: 0.9568, Prec: 0.9568, Rec: 0.9573, F1: 0.9569 | Val Loss: 3.1673, Val Acc: 0.6281, Prec: 0.3750, Rec: 0.3701, F1: 0.3686\nBest model saved with F1: 0.3686 at epoch 217\nEpoch 218/500: Train Loss: 0.1277, Train Acc: 0.9530, Prec: 0.9531, Rec: 0.9536, F1: 0.9531 | Val Loss: 3.1869, Val Acc: 0.5895, Prec: 0.3253, Rec: 0.3297, F1: 0.3226\nEpoch 219/500: Train Loss: 0.1131, Train Acc: 0.9594, Prec: 0.9595, Rec: 0.9600, F1: 0.9595 | Val Loss: 3.1965, Val Acc: 0.5649, Prec: 0.3235, Rec: 0.3296, F1: 0.3252\nEpoch 220/500: Train Loss: 0.1248, Train Acc: 0.9538, Prec: 0.9539, Rec: 0.9544, F1: 0.9538 | Val Loss: 3.1828, Val Acc: 0.5579, Prec: 0.3159, Rec: 0.3133, F1: 0.3085\nEpoch 221/500: Train Loss: 0.1085, Train Acc: 0.9602, Prec: 0.9603, Rec: 0.9607, F1: 0.9604 | Val Loss: 3.2153, Val Acc: 0.5895, Prec: 0.3342, Rec: 0.3304, F1: 0.3318\nEpoch 222/500: Train Loss: 0.1221, Train Acc: 0.9566, Prec: 0.9567, Rec: 0.9574, F1: 0.9567 | Val Loss: 3.1435, Val Acc: 0.5965, Prec: 0.3221, Rec: 0.3146, F1: 0.3161\nEpoch 223/500: Train Loss: 0.1185, Train Acc: 0.9594, Prec: 0.9595, Rec: 0.9599, F1: 0.9595 | Val Loss: 3.2558, Val Acc: 0.5965, Prec: 0.3146, Rec: 0.3112, F1: 0.3119\nEpoch 224/500: Train Loss: 0.1230, Train Acc: 0.9566, Prec: 0.9568, Rec: 0.9571, F1: 0.9567 | Val Loss: 3.1048, Val Acc: 0.5719, Prec: 0.3222, Rec: 0.3312, F1: 0.3220\nEpoch 225/500: Train Loss: 0.1129, Train Acc: 0.9612, Prec: 0.9612, Rec: 0.9618, F1: 0.9613 | Val Loss: 3.2299, Val Acc: 0.5789, Prec: 0.3412, Rec: 0.3444, F1: 0.3407\nEpoch 226/500: Train Loss: 0.1170, Train Acc: 0.9582, Prec: 0.9583, Rec: 0.9588, F1: 0.9583 | Val Loss: 3.3033, Val Acc: 0.5825, Prec: 0.3124, Rec: 0.3109, F1: 0.3113\nEpoch 227/500: Train Loss: 0.1137, Train Acc: 0.9600, Prec: 0.9601, Rec: 0.9606, F1: 0.9601 | Val Loss: 3.3647, Val Acc: 0.6035, Prec: 0.3391, Rec: 0.3278, F1: 0.3326\nEpoch 228/500: Train Loss: 0.1136, Train Acc: 0.9572, Prec: 0.9573, Rec: 0.9578, F1: 0.9573 | Val Loss: 3.3529, Val Acc: 0.5930, Prec: 0.3360, Rec: 0.3384, F1: 0.3365\nEpoch 229/500: Train Loss: 0.1122, Train Acc: 0.9596, Prec: 0.9596, Rec: 0.9601, F1: 0.9597 | Val Loss: 3.2814, Val Acc: 0.6246, Prec: 0.3463, Rec: 0.3406, F1: 0.3393\nEpoch 230/500: Train Loss: 0.1149, Train Acc: 0.9602, Prec: 0.9603, Rec: 0.9607, F1: 0.9603 | Val Loss: 3.3891, Val Acc: 0.6105, Prec: 0.3329, Rec: 0.3294, F1: 0.3288\nEpoch 231/500: Train Loss: 0.1229, Train Acc: 0.9546, Prec: 0.9547, Rec: 0.9552, F1: 0.9548 | Val Loss: 3.3376, Val Acc: 0.6281, Prec: 0.3341, Rec: 0.3276, F1: 0.3260\nEpoch 232/500: Train Loss: 0.1000, Train Acc: 0.9650, Prec: 0.9652, Rec: 0.9655, F1: 0.9652 | Val Loss: 3.3179, Val Acc: 0.5895, Prec: 0.3356, Rec: 0.3504, F1: 0.3405\nEpoch 233/500: Train Loss: 0.1039, Train Acc: 0.9646, Prec: 0.9648, Rec: 0.9651, F1: 0.9648 | Val Loss: 3.3206, Val Acc: 0.6175, Prec: 0.3714, Rec: 0.3498, F1: 0.3578\nEpoch 234/500: Train Loss: 0.1097, Train Acc: 0.9582, Prec: 0.9583, Rec: 0.9587, F1: 0.9583 | Val Loss: 3.2764, Val Acc: 0.5684, Prec: 0.3507, Rec: 0.3445, F1: 0.3387\nEpoch 235/500: Train Loss: 0.1218, Train Acc: 0.9574, Prec: 0.9575, Rec: 0.9579, F1: 0.9575 | Val Loss: 3.2340, Val Acc: 0.5895, Prec: 0.3214, Rec: 0.3294, F1: 0.3236\nEpoch 236/500: Train Loss: 0.1100, Train Acc: 0.9638, Prec: 0.9639, Rec: 0.9643, F1: 0.9639 | Val Loss: 3.3357, Val Acc: 0.5825, Prec: 0.3397, Rec: 0.3316, F1: 0.3335\nEpoch 237/500: Train Loss: 0.1080, Train Acc: 0.9618, Prec: 0.9619, Rec: 0.9623, F1: 0.9619 | Val Loss: 3.2490, Val Acc: 0.5789, Prec: 0.3424, Rec: 0.3440, F1: 0.3381\nEpoch 238/500: Train Loss: 0.1053, Train Acc: 0.9614, Prec: 0.9616, Rec: 0.9620, F1: 0.9615 | Val Loss: 3.3539, Val Acc: 0.5965, Prec: 0.3488, Rec: 0.3502, F1: 0.3487\nEpoch 239/500: Train Loss: 0.1075, Train Acc: 0.9594, Prec: 0.9594, Rec: 0.9599, F1: 0.9595 | Val Loss: 3.3929, Val Acc: 0.5930, Prec: 0.3240, Rec: 0.3236, F1: 0.3176\nEpoch 240/500: Train Loss: 0.1114, Train Acc: 0.9608, Prec: 0.9609, Rec: 0.9613, F1: 0.9609 | Val Loss: 3.2428, Val Acc: 0.5895, Prec: 0.3727, Rec: 0.3622, F1: 0.3651\nEpoch 241/500: Train Loss: 0.1086, Train Acc: 0.9592, Prec: 0.9592, Rec: 0.9598, F1: 0.9592 | Val Loss: 3.3362, Val Acc: 0.5614, Prec: 0.3135, Rec: 0.3179, F1: 0.3141\nEpoch 242/500: Train Loss: 0.1049, Train Acc: 0.9642, Prec: 0.9643, Rec: 0.9647, F1: 0.9643 | Val Loss: 3.4458, Val Acc: 0.5684, Prec: 0.3189, Rec: 0.3266, F1: 0.3160\nEpoch 243/500: Train Loss: 0.1021, Train Acc: 0.9630, Prec: 0.9631, Rec: 0.9635, F1: 0.9631 | Val Loss: 3.5455, Val Acc: 0.5754, Prec: 0.3188, Rec: 0.3184, F1: 0.3169\nEpoch 244/500: Train Loss: 0.1147, Train Acc: 0.9592, Prec: 0.9594, Rec: 0.9597, F1: 0.9593 | Val Loss: 3.3607, Val Acc: 0.5930, Prec: 0.3120, Rec: 0.3167, F1: 0.3114\nEpoch 245/500: Train Loss: 0.1076, Train Acc: 0.9634, Prec: 0.9635, Rec: 0.9639, F1: 0.9635 | Val Loss: 3.4774, Val Acc: 0.6035, Prec: 0.3539, Rec: 0.3285, F1: 0.3357\nEpoch 246/500: Train Loss: 0.1128, Train Acc: 0.9592, Prec: 0.9592, Rec: 0.9597, F1: 0.9593 | Val Loss: 3.3555, Val Acc: 0.5825, Prec: 0.3032, Rec: 0.3099, F1: 0.3030\nEpoch 247/500: Train Loss: 0.1037, Train Acc: 0.9604, Prec: 0.9606, Rec: 0.9609, F1: 0.9606 | Val Loss: 3.4457, Val Acc: 0.5895, Prec: 0.3486, Rec: 0.3581, F1: 0.3510\nEpoch 248/500: Train Loss: 0.1090, Train Acc: 0.9612, Prec: 0.9613, Rec: 0.9618, F1: 0.9613 | Val Loss: 3.3572, Val Acc: 0.5930, Prec: 0.3577, Rec: 0.3564, F1: 0.3559\nEpoch 249/500: Train Loss: 0.0966, Train Acc: 0.9654, Prec: 0.9655, Rec: 0.9659, F1: 0.9656 | Val Loss: 3.4455, Val Acc: 0.5684, Prec: 0.3232, Rec: 0.3266, F1: 0.3193\nEpoch 250/500: Train Loss: 0.1106, Train Acc: 0.9590, Prec: 0.9591, Rec: 0.9596, F1: 0.9591 | Val Loss: 3.2717, Val Acc: 0.5895, Prec: 0.3551, Rec: 0.3583, F1: 0.3553\nEpoch 251/500: Train Loss: 0.0996, Train Acc: 0.9630, Prec: 0.9631, Rec: 0.9635, F1: 0.9631 | Val Loss: 3.4868, Val Acc: 0.5965, Prec: 0.3306, Rec: 0.3354, F1: 0.3304\nEpoch 252/500: Train Loss: 0.1078, Train Acc: 0.9626, Prec: 0.9626, Rec: 0.9631, F1: 0.9627 | Val Loss: 3.4522, Val Acc: 0.6070, Prec: 0.3378, Rec: 0.3182, F1: 0.3229\nEpoch 253/500: Train Loss: 0.1004, Train Acc: 0.9652, Prec: 0.9653, Rec: 0.9656, F1: 0.9653 | Val Loss: 3.5722, Val Acc: 0.5649, Prec: 0.3311, Rec: 0.3231, F1: 0.3235\nEpoch 254/500: Train Loss: 0.1116, Train Acc: 0.9598, Prec: 0.9599, Rec: 0.9603, F1: 0.9599 | Val Loss: 3.4356, Val Acc: 0.5860, Prec: 0.3285, Rec: 0.3193, F1: 0.3212\nEpoch 255/500: Train Loss: 0.1138, Train Acc: 0.9586, Prec: 0.9587, Rec: 0.9591, F1: 0.9587 | Val Loss: 3.4034, Val Acc: 0.5860, Prec: 0.3642, Rec: 0.3749, F1: 0.3682\nEpoch 256/500: Train Loss: 0.0998, Train Acc: 0.9640, Prec: 0.9641, Rec: 0.9645, F1: 0.9641 | Val Loss: 3.3855, Val Acc: 0.5789, Prec: 0.3078, Rec: 0.3157, F1: 0.3069\nEpoch 257/500: Train Loss: 0.1023, Train Acc: 0.9636, Prec: 0.9637, Rec: 0.9641, F1: 0.9637 | Val Loss: 3.3935, Val Acc: 0.6035, Prec: 0.3449, Rec: 0.3420, F1: 0.3427\nEpoch 258/500: Train Loss: 0.1079, Train Acc: 0.9650, Prec: 0.9651, Rec: 0.9654, F1: 0.9651 | Val Loss: 3.5832, Val Acc: 0.5579, Prec: 0.3057, Rec: 0.3130, F1: 0.3065\nEpoch 259/500: Train Loss: 0.0966, Train Acc: 0.9680, Prec: 0.9681, Rec: 0.9684, F1: 0.9682 | Val Loss: 3.4266, Val Acc: 0.5860, Prec: 0.3436, Rec: 0.3569, F1: 0.3482\nEpoch 260/500: Train Loss: 0.1092, Train Acc: 0.9640, Prec: 0.9640, Rec: 0.9644, F1: 0.9641 | Val Loss: 3.5100, Val Acc: 0.6351, Prec: 0.3865, Rec: 0.3591, F1: 0.3695\nBest model saved with F1: 0.3695 at epoch 260\nEpoch 261/500: Train Loss: 0.1114, Train Acc: 0.9614, Prec: 0.9616, Rec: 0.9619, F1: 0.9616 | Val Loss: 3.5253, Val Acc: 0.6070, Prec: 0.3457, Rec: 0.3355, F1: 0.3350\nEpoch 262/500: Train Loss: 0.0873, Train Acc: 0.9704, Prec: 0.9705, Rec: 0.9707, F1: 0.9705 | Val Loss: 3.5534, Val Acc: 0.5474, Prec: 0.3106, Rec: 0.3201, F1: 0.3120\nEpoch 263/500: Train Loss: 0.0991, Train Acc: 0.9640, Prec: 0.9642, Rec: 0.9645, F1: 0.9641 | Val Loss: 3.6750, Val Acc: 0.6000, Prec: 0.3407, Rec: 0.3399, F1: 0.3330\nEpoch 264/500: Train Loss: 0.1074, Train Acc: 0.9620, Prec: 0.9621, Rec: 0.9624, F1: 0.9621 | Val Loss: 3.5680, Val Acc: 0.5965, Prec: 0.3660, Rec: 0.3744, F1: 0.3681\nEpoch 265/500: Train Loss: 0.0977, Train Acc: 0.9654, Prec: 0.9655, Rec: 0.9658, F1: 0.9656 | Val Loss: 3.5618, Val Acc: 0.6000, Prec: 0.3465, Rec: 0.3444, F1: 0.3452\nEpoch 266/500: Train Loss: 0.0927, Train Acc: 0.9688, Prec: 0.9689, Rec: 0.9692, F1: 0.9689 | Val Loss: 3.7486, Val Acc: 0.5263, Prec: 0.3104, Rec: 0.3211, F1: 0.3109\nEpoch 267/500: Train Loss: 0.1121, Train Acc: 0.9596, Prec: 0.9596, Rec: 0.9601, F1: 0.9597 | Val Loss: 3.4660, Val Acc: 0.5614, Prec: 0.3367, Rec: 0.3493, F1: 0.3401\nEpoch 268/500: Train Loss: 0.0975, Train Acc: 0.9676, Prec: 0.9678, Rec: 0.9681, F1: 0.9677 | Val Loss: 3.5079, Val Acc: 0.5895, Prec: 0.3429, Rec: 0.3407, F1: 0.3397\nEpoch 269/500: Train Loss: 0.0888, Train Acc: 0.9704, Prec: 0.9705, Rec: 0.9708, F1: 0.9705 | Val Loss: 3.5120, Val Acc: 0.6175, Prec: 0.3607, Rec: 0.3457, F1: 0.3485\nEpoch 270/500: Train Loss: 0.1053, Train Acc: 0.9628, Prec: 0.9629, Rec: 0.9632, F1: 0.9629 | Val Loss: 3.5431, Val Acc: 0.5930, Prec: 0.3334, Rec: 0.3346, F1: 0.3325\nEpoch 271/500: Train Loss: 0.1092, Train Acc: 0.9628, Prec: 0.9629, Rec: 0.9634, F1: 0.9629 | Val Loss: 3.4898, Val Acc: 0.6526, Prec: 0.3730, Rec: 0.3425, F1: 0.3498\nEpoch 272/500: Train Loss: 0.1006, Train Acc: 0.9642, Prec: 0.9643, Rec: 0.9647, F1: 0.9643 | Val Loss: 3.5078, Val Acc: 0.6070, Prec: 0.3375, Rec: 0.3421, F1: 0.3365\nEpoch 273/500: Train Loss: 0.0922, Train Acc: 0.9644, Prec: 0.9645, Rec: 0.9647, F1: 0.9645 | Val Loss: 3.6134, Val Acc: 0.5754, Prec: 0.3065, Rec: 0.3216, F1: 0.3116\nEpoch 274/500: Train Loss: 0.0897, Train Acc: 0.9666, Prec: 0.9667, Rec: 0.9670, F1: 0.9667 | Val Loss: 3.5170, Val Acc: 0.6035, Prec: 0.3368, Rec: 0.3379, F1: 0.3366\nEpoch 275/500: Train Loss: 0.0952, Train Acc: 0.9656, Prec: 0.9657, Rec: 0.9661, F1: 0.9657 | Val Loss: 3.6054, Val Acc: 0.5789, Prec: 0.3252, Rec: 0.3299, F1: 0.3257\nEpoch 276/500: Train Loss: 0.1090, Train Acc: 0.9614, Prec: 0.9614, Rec: 0.9618, F1: 0.9615 | Val Loss: 3.5544, Val Acc: 0.6070, Prec: 0.3285, Rec: 0.3148, F1: 0.3202\nEpoch 277/500: Train Loss: 0.0979, Train Acc: 0.9650, Prec: 0.9651, Rec: 0.9655, F1: 0.9651 | Val Loss: 3.4336, Val Acc: 0.6035, Prec: 0.3552, Rec: 0.3520, F1: 0.3533\nEpoch 278/500: Train Loss: 0.0960, Train Acc: 0.9634, Prec: 0.9635, Rec: 0.9639, F1: 0.9635 | Val Loss: 3.6563, Val Acc: 0.6316, Prec: 0.3845, Rec: 0.3545, F1: 0.3658\nEpoch 279/500: Train Loss: 0.0881, Train Acc: 0.9692, Prec: 0.9692, Rec: 0.9696, F1: 0.9693 | Val Loss: 3.6818, Val Acc: 0.5895, Prec: 0.3237, Rec: 0.3297, F1: 0.3227\nEpoch 280/500: Train Loss: 0.0942, Train Acc: 0.9652, Prec: 0.9652, Rec: 0.9657, F1: 0.9654 | Val Loss: 3.5753, Val Acc: 0.5965, Prec: 0.3358, Rec: 0.3391, F1: 0.3368\nEpoch 281/500: Train Loss: 0.0999, Train Acc: 0.9666, Prec: 0.9667, Rec: 0.9670, F1: 0.9668 | Val Loss: 3.5897, Val Acc: 0.5930, Prec: 0.3337, Rec: 0.3311, F1: 0.3319\nEpoch 282/500: Train Loss: 0.0991, Train Acc: 0.9676, Prec: 0.9677, Rec: 0.9681, F1: 0.9677 | Val Loss: 3.4422, Val Acc: 0.5895, Prec: 0.3135, Rec: 0.3225, F1: 0.3149\nEpoch 283/500: Train Loss: 0.0891, Train Acc: 0.9692, Prec: 0.9693, Rec: 0.9697, F1: 0.9693 | Val Loss: 3.5144, Val Acc: 0.6035, Prec: 0.3443, Rec: 0.3482, F1: 0.3448\nEpoch 284/500: Train Loss: 0.0876, Train Acc: 0.9700, Prec: 0.9701, Rec: 0.9703, F1: 0.9701 | Val Loss: 3.6317, Val Acc: 0.5719, Prec: 0.3158, Rec: 0.3208, F1: 0.3180\nEpoch 285/500: Train Loss: 0.0937, Train Acc: 0.9638, Prec: 0.9639, Rec: 0.9642, F1: 0.9639 | Val Loss: 3.6637, Val Acc: 0.6140, Prec: 0.3444, Rec: 0.3311, F1: 0.3363\nEpoch 286/500: Train Loss: 0.0818, Train Acc: 0.9724, Prec: 0.9725, Rec: 0.9728, F1: 0.9726 | Val Loss: 3.5711, Val Acc: 0.6211, Prec: 0.3729, Rec: 0.3748, F1: 0.3723\nBest model saved with F1: 0.3723 at epoch 286\nEpoch 287/500: Train Loss: 0.0953, Train Acc: 0.9668, Prec: 0.9669, Rec: 0.9672, F1: 0.9670 | Val Loss: 3.7199, Val Acc: 0.5895, Prec: 0.3444, Rec: 0.3508, F1: 0.3444\nEpoch 288/500: Train Loss: 0.0911, Train Acc: 0.9668, Prec: 0.9669, Rec: 0.9672, F1: 0.9669 | Val Loss: 3.6580, Val Acc: 0.5965, Prec: 0.3358, Rec: 0.3326, F1: 0.3328\nEpoch 289/500: Train Loss: 0.0954, Train Acc: 0.9652, Prec: 0.9653, Rec: 0.9656, F1: 0.9653 | Val Loss: 3.6590, Val Acc: 0.5649, Prec: 0.3220, Rec: 0.3224, F1: 0.3204\nEpoch 290/500: Train Loss: 0.0905, Train Acc: 0.9690, Prec: 0.9690, Rec: 0.9695, F1: 0.9691 | Val Loss: 3.7394, Val Acc: 0.6316, Prec: 0.3690, Rec: 0.3535, F1: 0.3568\nEpoch 291/500: Train Loss: 0.0942, Train Acc: 0.9652, Prec: 0.9652, Rec: 0.9657, F1: 0.9653 | Val Loss: 3.5477, Val Acc: 0.5965, Prec: 0.3255, Rec: 0.3250, F1: 0.3250\nEpoch 292/500: Train Loss: 0.0976, Train Acc: 0.9658, Prec: 0.9660, Rec: 0.9663, F1: 0.9660 | Val Loss: 3.6902, Val Acc: 0.6246, Prec: 0.3709, Rec: 0.3721, F1: 0.3684\nEpoch 293/500: Train Loss: 0.0969, Train Acc: 0.9670, Prec: 0.9671, Rec: 0.9674, F1: 0.9671 | Val Loss: 3.7104, Val Acc: 0.6105, Prec: 0.3555, Rec: 0.3435, F1: 0.3470\nEpoch 294/500: Train Loss: 0.0825, Train Acc: 0.9722, Prec: 0.9723, Rec: 0.9726, F1: 0.9723 | Val Loss: 3.5818, Val Acc: 0.6035, Prec: 0.3533, Rec: 0.3479, F1: 0.3476\nEpoch 295/500: Train Loss: 0.0903, Train Acc: 0.9670, Prec: 0.9671, Rec: 0.9674, F1: 0.9671 | Val Loss: 3.7322, Val Acc: 0.5965, Prec: 0.3230, Rec: 0.3250, F1: 0.3219\nEpoch 296/500: Train Loss: 0.0927, Train Acc: 0.9654, Prec: 0.9655, Rec: 0.9659, F1: 0.9655 | Val Loss: 3.7646, Val Acc: 0.5719, Prec: 0.3269, Rec: 0.3315, F1: 0.3272\nEpoch 297/500: Train Loss: 0.0777, Train Acc: 0.9736, Prec: 0.9736, Rec: 0.9740, F1: 0.9737 | Val Loss: 3.6492, Val Acc: 0.5825, Prec: 0.3343, Rec: 0.3348, F1: 0.3339\nEpoch 298/500: Train Loss: 0.0910, Train Acc: 0.9684, Prec: 0.9685, Rec: 0.9689, F1: 0.9686 | Val Loss: 3.7228, Val Acc: 0.6035, Prec: 0.3499, Rec: 0.3449, F1: 0.3463\nEpoch 299/500: Train Loss: 0.1039, Train Acc: 0.9594, Prec: 0.9595, Rec: 0.9599, F1: 0.9595 | Val Loss: 3.6036, Val Acc: 0.5754, Prec: 0.3402, Rec: 0.3503, F1: 0.3442\nEpoch 300/500: Train Loss: 0.0875, Train Acc: 0.9650, Prec: 0.9651, Rec: 0.9655, F1: 0.9651 | Val Loss: 3.7422, Val Acc: 0.6316, Prec: 0.3524, Rec: 0.3328, F1: 0.3401\nEpoch 301/500: Train Loss: 0.0898, Train Acc: 0.9644, Prec: 0.9645, Rec: 0.9648, F1: 0.9646 | Val Loss: 3.5937, Val Acc: 0.6105, Prec: 0.3905, Rec: 0.3864, F1: 0.3873\nBest model saved with F1: 0.3873 at epoch 301\nEpoch 302/500: Train Loss: 0.0863, Train Acc: 0.9718, Prec: 0.9718, Rec: 0.9721, F1: 0.9719 | Val Loss: 3.6200, Val Acc: 0.6070, Prec: 0.3737, Rec: 0.3749, F1: 0.3739\nEpoch 303/500: Train Loss: 0.0891, Train Acc: 0.9674, Prec: 0.9674, Rec: 0.9679, F1: 0.9675 | Val Loss: 3.7040, Val Acc: 0.6140, Prec: 0.3408, Rec: 0.3339, F1: 0.3368\nEpoch 304/500: Train Loss: 0.0771, Train Acc: 0.9736, Prec: 0.9737, Rec: 0.9740, F1: 0.9737 | Val Loss: 3.7762, Val Acc: 0.6526, Prec: 0.3924, Rec: 0.3843, F1: 0.3859\nEpoch 305/500: Train Loss: 0.0831, Train Acc: 0.9706, Prec: 0.9707, Rec: 0.9710, F1: 0.9707 | Val Loss: 3.7555, Val Acc: 0.6175, Prec: 0.3620, Rec: 0.3596, F1: 0.3600\nEpoch 306/500: Train Loss: 0.0914, Train Acc: 0.9666, Prec: 0.9666, Rec: 0.9671, F1: 0.9667 | Val Loss: 3.8032, Val Acc: 0.6035, Prec: 0.3362, Rec: 0.3310, F1: 0.3330\nEpoch 307/500: Train Loss: 0.0825, Train Acc: 0.9712, Prec: 0.9713, Rec: 0.9717, F1: 0.9714 | Val Loss: 3.9874, Val Acc: 0.5719, Prec: 0.3540, Rec: 0.3325, F1: 0.3347\nEpoch 308/500: Train Loss: 0.0967, Train Acc: 0.9644, Prec: 0.9644, Rec: 0.9647, F1: 0.9645 | Val Loss: 3.7534, Val Acc: 0.5860, Prec: 0.3259, Rec: 0.3286, F1: 0.3270\nEpoch 309/500: Train Loss: 0.0967, Train Acc: 0.9670, Prec: 0.9670, Rec: 0.9674, F1: 0.9671 | Val Loss: 3.7997, Val Acc: 0.6035, Prec: 0.3337, Rec: 0.3306, F1: 0.3284\nEpoch 310/500: Train Loss: 0.0892, Train Acc: 0.9684, Prec: 0.9685, Rec: 0.9688, F1: 0.9685 | Val Loss: 3.6705, Val Acc: 0.6351, Prec: 0.3480, Rec: 0.3232, F1: 0.3250\nEpoch 311/500: Train Loss: 0.0846, Train Acc: 0.9712, Prec: 0.9713, Rec: 0.9716, F1: 0.9713 | Val Loss: 3.7430, Val Acc: 0.5930, Prec: 0.3546, Rec: 0.3560, F1: 0.3524\nEpoch 312/500: Train Loss: 0.0869, Train Acc: 0.9692, Prec: 0.9692, Rec: 0.9696, F1: 0.9693 | Val Loss: 3.8396, Val Acc: 0.6316, Prec: 0.3704, Rec: 0.3576, F1: 0.3631\nEpoch 313/500: Train Loss: 0.1002, Train Acc: 0.9634, Prec: 0.9634, Rec: 0.9638, F1: 0.9635 | Val Loss: 3.7163, Val Acc: 0.5825, Prec: 0.3574, Rec: 0.3531, F1: 0.3522\nEpoch 314/500: Train Loss: 0.0819, Train Acc: 0.9692, Prec: 0.9693, Rec: 0.9697, F1: 0.9694 | Val Loss: 3.6825, Val Acc: 0.5965, Prec: 0.3600, Rec: 0.3534, F1: 0.3546\nEpoch 315/500: Train Loss: 0.0888, Train Acc: 0.9660, Prec: 0.9660, Rec: 0.9665, F1: 0.9661 | Val Loss: 3.7599, Val Acc: 0.6246, Prec: 0.3629, Rec: 0.3347, F1: 0.3454\nEpoch 316/500: Train Loss: 0.0838, Train Acc: 0.9702, Prec: 0.9703, Rec: 0.9707, F1: 0.9704 | Val Loss: 3.7105, Val Acc: 0.6140, Prec: 0.3611, Rec: 0.3519, F1: 0.3549\nEpoch 317/500: Train Loss: 0.0789, Train Acc: 0.9728, Prec: 0.9729, Rec: 0.9731, F1: 0.9729 | Val Loss: 3.9917, Val Acc: 0.5754, Prec: 0.3216, Rec: 0.3219, F1: 0.3193\nEpoch 318/500: Train Loss: 0.0885, Train Acc: 0.9694, Prec: 0.9694, Rec: 0.9698, F1: 0.9695 | Val Loss: 4.0108, Val Acc: 0.6035, Prec: 0.3591, Rec: 0.3417, F1: 0.3469\nEpoch 319/500: Train Loss: 0.0817, Train Acc: 0.9736, Prec: 0.9736, Rec: 0.9739, F1: 0.9737 | Val Loss: 3.8266, Val Acc: 0.5825, Prec: 0.3078, Rec: 0.3103, F1: 0.3079\nEpoch 320/500: Train Loss: 0.0721, Train Acc: 0.9736, Prec: 0.9737, Rec: 0.9739, F1: 0.9738 | Val Loss: 4.0893, Val Acc: 0.6000, Prec: 0.3641, Rec: 0.3721, F1: 0.3661\nEpoch 321/500: Train Loss: 0.0899, Train Acc: 0.9682, Prec: 0.9683, Rec: 0.9686, F1: 0.9683 | Val Loss: 3.8362, Val Acc: 0.6035, Prec: 0.3271, Rec: 0.3203, F1: 0.3219\nEpoch 322/500: Train Loss: 0.0831, Train Acc: 0.9720, Prec: 0.9720, Rec: 0.9724, F1: 0.9721 | Val Loss: 3.8161, Val Acc: 0.6281, Prec: 0.3493, Rec: 0.3320, F1: 0.3387\nEpoch 323/500: Train Loss: 0.0976, Train Acc: 0.9642, Prec: 0.9643, Rec: 0.9646, F1: 0.9644 | Val Loss: 3.8182, Val Acc: 0.5860, Prec: 0.3384, Rec: 0.3396, F1: 0.3367\nEpoch 324/500: Train Loss: 0.0842, Train Acc: 0.9704, Prec: 0.9705, Rec: 0.9708, F1: 0.9706 | Val Loss: 3.8452, Val Acc: 0.6000, Prec: 0.3474, Rec: 0.3475, F1: 0.3456\nEpoch 325/500: Train Loss: 0.0802, Train Acc: 0.9708, Prec: 0.9708, Rec: 0.9712, F1: 0.9709 | Val Loss: 4.0664, Val Acc: 0.6105, Prec: 0.3283, Rec: 0.3325, F1: 0.3248\nEpoch 326/500: Train Loss: 0.0863, Train Acc: 0.9702, Prec: 0.9702, Rec: 0.9705, F1: 0.9703 | Val Loss: 3.9580, Val Acc: 0.6140, Prec: 0.3572, Rec: 0.3487, F1: 0.3513\nEpoch 327/500: Train Loss: 0.0824, Train Acc: 0.9722, Prec: 0.9723, Rec: 0.9727, F1: 0.9724 | Val Loss: 3.8628, Val Acc: 0.6105, Prec: 0.3526, Rec: 0.3366, F1: 0.3409\nEpoch 328/500: Train Loss: 0.0789, Train Acc: 0.9722, Prec: 0.9722, Rec: 0.9726, F1: 0.9723 | Val Loss: 3.9486, Val Acc: 0.6281, Prec: 0.3545, Rec: 0.3314, F1: 0.3379\nEpoch 329/500: Train Loss: 0.0768, Train Acc: 0.9718, Prec: 0.9719, Rec: 0.9723, F1: 0.9720 | Val Loss: 3.9915, Val Acc: 0.6070, Prec: 0.3378, Rec: 0.3352, F1: 0.3338\nEpoch 330/500: Train Loss: 0.0784, Train Acc: 0.9714, Prec: 0.9715, Rec: 0.9718, F1: 0.9715 | Val Loss: 3.9982, Val Acc: 0.6211, Prec: 0.3526, Rec: 0.3330, F1: 0.3380\nEpoch 331/500: Train Loss: 0.0883, Train Acc: 0.9692, Prec: 0.9693, Rec: 0.9696, F1: 0.9693 | Val Loss: 3.9466, Val Acc: 0.6000, Prec: 0.3333, Rec: 0.3226, F1: 0.3248\nEpoch 332/500: Train Loss: 0.0815, Train Acc: 0.9698, Prec: 0.9699, Rec: 0.9702, F1: 0.9700 | Val Loss: 3.9762, Val Acc: 0.6105, Prec: 0.3594, Rec: 0.3512, F1: 0.3538\nEpoch 333/500: Train Loss: 0.0801, Train Acc: 0.9686, Prec: 0.9686, Rec: 0.9689, F1: 0.9687 | Val Loss: 3.7309, Val Acc: 0.6175, Prec: 0.3904, Rec: 0.3671, F1: 0.3705\nEpoch 334/500: Train Loss: 0.0813, Train Acc: 0.9710, Prec: 0.9711, Rec: 0.9713, F1: 0.9711 | Val Loss: 4.0339, Val Acc: 0.6070, Prec: 0.3604, Rec: 0.3667, F1: 0.3619\nEpoch 335/500: Train Loss: 0.0749, Train Acc: 0.9758, Prec: 0.9759, Rec: 0.9762, F1: 0.9759 | Val Loss: 3.9506, Val Acc: 0.6351, Prec: 0.3775, Rec: 0.3480, F1: 0.3579\nEpoch 336/500: Train Loss: 0.0721, Train Acc: 0.9728, Prec: 0.9728, Rec: 0.9731, F1: 0.9729 | Val Loss: 3.9576, Val Acc: 0.6140, Prec: 0.3571, Rec: 0.3518, F1: 0.3535\nEpoch 337/500: Train Loss: 0.0905, Train Acc: 0.9674, Prec: 0.9674, Rec: 0.9678, F1: 0.9675 | Val Loss: 3.7862, Val Acc: 0.6105, Prec: 0.3396, Rec: 0.3294, F1: 0.3328\nEpoch 338/500: Train Loss: 0.0968, Train Acc: 0.9644, Prec: 0.9645, Rec: 0.9649, F1: 0.9646 | Val Loss: 3.8476, Val Acc: 0.6140, Prec: 0.3584, Rec: 0.3384, F1: 0.3448\nEpoch 339/500: Train Loss: 0.0760, Train Acc: 0.9722, Prec: 0.9723, Rec: 0.9726, F1: 0.9723 | Val Loss: 3.8830, Val Acc: 0.6035, Prec: 0.3722, Rec: 0.3766, F1: 0.3737\nEpoch 340/500: Train Loss: 0.0709, Train Acc: 0.9760, Prec: 0.9761, Rec: 0.9763, F1: 0.9761 | Val Loss: 4.0462, Val Acc: 0.6035, Prec: 0.3467, Rec: 0.3583, F1: 0.3474\nEpoch 341/500: Train Loss: 0.0788, Train Acc: 0.9700, Prec: 0.9701, Rec: 0.9704, F1: 0.9701 | Val Loss: 3.9326, Val Acc: 0.6105, Prec: 0.3693, Rec: 0.3612, F1: 0.3646\nEpoch 342/500: Train Loss: 0.0906, Train Acc: 0.9680, Prec: 0.9681, Rec: 0.9684, F1: 0.9682 | Val Loss: 3.9197, Val Acc: 0.6175, Prec: 0.3731, Rec: 0.3606, F1: 0.3654\nEpoch 343/500: Train Loss: 0.0775, Train Acc: 0.9736, Prec: 0.9737, Rec: 0.9740, F1: 0.9737 | Val Loss: 4.1514, Val Acc: 0.5825, Prec: 0.3297, Rec: 0.3344, F1: 0.3274\nEpoch 344/500: Train Loss: 0.0708, Train Acc: 0.9754, Prec: 0.9755, Rec: 0.9757, F1: 0.9755 | Val Loss: 3.9773, Val Acc: 0.6351, Prec: 0.3930, Rec: 0.3761, F1: 0.3836\nEpoch 345/500: Train Loss: 0.0768, Train Acc: 0.9744, Prec: 0.9745, Rec: 0.9747, F1: 0.9745 | Val Loss: 4.0639, Val Acc: 0.5930, Prec: 0.3460, Rec: 0.3494, F1: 0.3470\nEpoch 346/500: Train Loss: 0.0891, Train Acc: 0.9668, Prec: 0.9670, Rec: 0.9672, F1: 0.9670 | Val Loss: 4.0246, Val Acc: 0.5895, Prec: 0.3293, Rec: 0.3300, F1: 0.3286\nEpoch 347/500: Train Loss: 0.0878, Train Acc: 0.9662, Prec: 0.9662, Rec: 0.9666, F1: 0.9663 | Val Loss: 3.8168, Val Acc: 0.6316, Prec: 0.3758, Rec: 0.3709, F1: 0.3682\nEpoch 348/500: Train Loss: 0.0679, Train Acc: 0.9752, Prec: 0.9753, Rec: 0.9756, F1: 0.9753 | Val Loss: 4.1629, Val Acc: 0.6105, Prec: 0.3559, Rec: 0.3539, F1: 0.3537\nEpoch 349/500: Train Loss: 0.0739, Train Acc: 0.9730, Prec: 0.9731, Rec: 0.9734, F1: 0.9732 | Val Loss: 3.9581, Val Acc: 0.6000, Prec: 0.3663, Rec: 0.3721, F1: 0.3657\nEpoch 350/500: Train Loss: 0.0752, Train Acc: 0.9738, Prec: 0.9739, Rec: 0.9742, F1: 0.9740 | Val Loss: 4.1025, Val Acc: 0.5965, Prec: 0.3615, Rec: 0.3631, F1: 0.3555\nEpoch 351/500: Train Loss: 0.0821, Train Acc: 0.9710, Prec: 0.9710, Rec: 0.9714, F1: 0.9711 | Val Loss: 4.0041, Val Acc: 0.6140, Prec: 0.3440, Rec: 0.3235, F1: 0.3281\nEpoch 352/500: Train Loss: 0.0715, Train Acc: 0.9758, Prec: 0.9759, Rec: 0.9761, F1: 0.9759 | Val Loss: 3.9562, Val Acc: 0.6105, Prec: 0.3818, Rec: 0.3722, F1: 0.3762\nEpoch 353/500: Train Loss: 0.0796, Train Acc: 0.9714, Prec: 0.9714, Rec: 0.9718, F1: 0.9715 | Val Loss: 3.9799, Val Acc: 0.6175, Prec: 0.3596, Rec: 0.3427, F1: 0.3489\nEpoch 354/500: Train Loss: 0.0747, Train Acc: 0.9702, Prec: 0.9703, Rec: 0.9705, F1: 0.9704 | Val Loss: 4.0047, Val Acc: 0.5789, Prec: 0.3153, Rec: 0.3195, F1: 0.3159\nEpoch 355/500: Train Loss: 0.0893, Train Acc: 0.9682, Prec: 0.9683, Rec: 0.9686, F1: 0.9683 | Val Loss: 3.8917, Val Acc: 0.5965, Prec: 0.3344, Rec: 0.3284, F1: 0.3305\nEpoch 356/500: Train Loss: 0.0826, Train Acc: 0.9698, Prec: 0.9699, Rec: 0.9702, F1: 0.9699 | Val Loss: 3.9661, Val Acc: 0.6070, Prec: 0.3669, Rec: 0.3604, F1: 0.3620\nEpoch 357/500: Train Loss: 0.0696, Train Acc: 0.9752, Prec: 0.9752, Rec: 0.9756, F1: 0.9753 | Val Loss: 4.1114, Val Acc: 0.6316, Prec: 0.3700, Rec: 0.3435, F1: 0.3530\nEpoch 358/500: Train Loss: 0.0853, Train Acc: 0.9694, Prec: 0.9696, Rec: 0.9697, F1: 0.9695 | Val Loss: 3.9505, Val Acc: 0.5930, Prec: 0.3431, Rec: 0.3284, F1: 0.3326\nEpoch 359/500: Train Loss: 0.0733, Train Acc: 0.9740, Prec: 0.9740, Rec: 0.9743, F1: 0.9741 | Val Loss: 3.9641, Val Acc: 0.5860, Prec: 0.3459, Rec: 0.3466, F1: 0.3444\nEpoch 360/500: Train Loss: 0.0749, Train Acc: 0.9750, Prec: 0.9751, Rec: 0.9754, F1: 0.9751 | Val Loss: 4.1053, Val Acc: 0.6035, Prec: 0.3477, Rec: 0.3282, F1: 0.3354\nEpoch 361/500: Train Loss: 0.0721, Train Acc: 0.9732, Prec: 0.9733, Rec: 0.9735, F1: 0.9733 | Val Loss: 4.0451, Val Acc: 0.6140, Prec: 0.3557, Rec: 0.3453, F1: 0.3495\nEpoch 362/500: Train Loss: 0.0774, Train Acc: 0.9736, Prec: 0.9736, Rec: 0.9739, F1: 0.9737 | Val Loss: 4.0626, Val Acc: 0.5930, Prec: 0.3395, Rec: 0.3453, F1: 0.3407\nEpoch 363/500: Train Loss: 0.0741, Train Acc: 0.9720, Prec: 0.9721, Rec: 0.9724, F1: 0.9722 | Val Loss: 4.1315, Val Acc: 0.6105, Prec: 0.3422, Rec: 0.3266, F1: 0.3306\nEpoch 364/500: Train Loss: 0.0767, Train Acc: 0.9728, Prec: 0.9728, Rec: 0.9732, F1: 0.9729 | Val Loss: 3.9987, Val Acc: 0.5789, Prec: 0.3304, Rec: 0.3369, F1: 0.3322\nEpoch 365/500: Train Loss: 0.0771, Train Acc: 0.9726, Prec: 0.9727, Rec: 0.9729, F1: 0.9727 | Val Loss: 3.9647, Val Acc: 0.5754, Prec: 0.3518, Rec: 0.3578, F1: 0.3537\nEpoch 366/500: Train Loss: 0.0807, Train Acc: 0.9746, Prec: 0.9747, Rec: 0.9749, F1: 0.9747 | Val Loss: 4.0859, Val Acc: 0.6035, Prec: 0.3216, Rec: 0.3203, F1: 0.3183\nEpoch 367/500: Train Loss: 0.0787, Train Acc: 0.9700, Prec: 0.9701, Rec: 0.9703, F1: 0.9702 | Val Loss: 3.9762, Val Acc: 0.6211, Prec: 0.3615, Rec: 0.3540, F1: 0.3566\nEpoch 368/500: Train Loss: 0.0757, Train Acc: 0.9730, Prec: 0.9731, Rec: 0.9733, F1: 0.9732 | Val Loss: 3.9933, Val Acc: 0.6281, Prec: 0.3750, Rec: 0.3669, F1: 0.3704\nEpoch 369/500: Train Loss: 0.0730, Train Acc: 0.9748, Prec: 0.9748, Rec: 0.9752, F1: 0.9749 | Val Loss: 4.1189, Val Acc: 0.6035, Prec: 0.3520, Rec: 0.3451, F1: 0.3442\nEpoch 370/500: Train Loss: 0.0708, Train Acc: 0.9740, Prec: 0.9741, Rec: 0.9743, F1: 0.9741 | Val Loss: 4.2002, Val Acc: 0.5754, Prec: 0.3133, Rec: 0.3220, F1: 0.3167\nEpoch 371/500: Train Loss: 0.0725, Train Acc: 0.9754, Prec: 0.9755, Rec: 0.9757, F1: 0.9755 | Val Loss: 4.2067, Val Acc: 0.6246, Prec: 0.3595, Rec: 0.3379, F1: 0.3461\nEpoch 372/500: Train Loss: 0.0572, Train Acc: 0.9808, Prec: 0.9808, Rec: 0.9811, F1: 0.9809 | Val Loss: 3.9729, Val Acc: 0.6105, Prec: 0.3778, Rec: 0.3722, F1: 0.3742\nEpoch 373/500: Train Loss: 0.0686, Train Acc: 0.9770, Prec: 0.9770, Rec: 0.9772, F1: 0.9771 | Val Loss: 4.2262, Val Acc: 0.6175, Prec: 0.3636, Rec: 0.3599, F1: 0.3592\nEpoch 374/500: Train Loss: 0.0928, Train Acc: 0.9636, Prec: 0.9636, Rec: 0.9640, F1: 0.9637 | Val Loss: 4.2415, Val Acc: 0.6281, Prec: 0.3751, Rec: 0.3767, F1: 0.3718\nEpoch 375/500: Train Loss: 0.0768, Train Acc: 0.9738, Prec: 0.9739, Rec: 0.9742, F1: 0.9740 | Val Loss: 4.1217, Val Acc: 0.6281, Prec: 0.3837, Rec: 0.3770, F1: 0.3775\nEpoch 376/500: Train Loss: 0.0711, Train Acc: 0.9750, Prec: 0.9751, Rec: 0.9754, F1: 0.9752 | Val Loss: 4.1479, Val Acc: 0.6140, Prec: 0.3474, Rec: 0.3512, F1: 0.3461\nEpoch 377/500: Train Loss: 0.0569, Train Acc: 0.9794, Prec: 0.9795, Rec: 0.9798, F1: 0.9796 | Val Loss: 4.2746, Val Acc: 0.6035, Prec: 0.3437, Rec: 0.3310, F1: 0.3357\nEpoch 378/500: Train Loss: 0.0700, Train Acc: 0.9756, Prec: 0.9757, Rec: 0.9759, F1: 0.9757 | Val Loss: 4.3013, Val Acc: 0.6211, Prec: 0.3567, Rec: 0.3402, F1: 0.3472\nEpoch 379/500: Train Loss: 0.0768, Train Acc: 0.9724, Prec: 0.9725, Rec: 0.9727, F1: 0.9725 | Val Loss: 4.1570, Val Acc: 0.6140, Prec: 0.3501, Rec: 0.3415, F1: 0.3448\nEpoch 380/500: Train Loss: 0.0833, Train Acc: 0.9708, Prec: 0.9708, Rec: 0.9711, F1: 0.9709 | Val Loss: 4.2149, Val Acc: 0.6421, Prec: 0.3862, Rec: 0.3676, F1: 0.3726\nEpoch 381/500: Train Loss: 0.0762, Train Acc: 0.9716, Prec: 0.9716, Rec: 0.9720, F1: 0.9717 | Val Loss: 4.1634, Val Acc: 0.6211, Prec: 0.3460, Rec: 0.3395, F1: 0.3412\nEpoch 382/500: Train Loss: 0.0720, Train Acc: 0.9748, Prec: 0.9749, Rec: 0.9751, F1: 0.9749 | Val Loss: 4.1404, Val Acc: 0.6175, Prec: 0.3776, Rec: 0.3806, F1: 0.3778\nEpoch 383/500: Train Loss: 0.0705, Train Acc: 0.9758, Prec: 0.9759, Rec: 0.9761, F1: 0.9759 | Val Loss: 4.3008, Val Acc: 0.6421, Prec: 0.3854, Rec: 0.3647, F1: 0.3731\nEpoch 384/500: Train Loss: 0.0654, Train Acc: 0.9754, Prec: 0.9755, Rec: 0.9758, F1: 0.9755 | Val Loss: 4.1501, Val Acc: 0.6175, Prec: 0.3591, Rec: 0.3429, F1: 0.3489\nEpoch 385/500: Train Loss: 0.0721, Train Acc: 0.9748, Prec: 0.9749, Rec: 0.9751, F1: 0.9750 | Val Loss: 4.1565, Val Acc: 0.6105, Prec: 0.3383, Rec: 0.3397, F1: 0.3384\nEpoch 386/500: Train Loss: 0.0719, Train Acc: 0.9748, Prec: 0.9748, Rec: 0.9751, F1: 0.9749 | Val Loss: 4.2999, Val Acc: 0.6000, Prec: 0.3599, Rec: 0.3582, F1: 0.3586\nEpoch 387/500: Train Loss: 0.0682, Train Acc: 0.9764, Prec: 0.9764, Rec: 0.9767, F1: 0.9764 | Val Loss: 4.3117, Val Acc: 0.5965, Prec: 0.3359, Rec: 0.3388, F1: 0.3344\nEpoch 388/500: Train Loss: 0.0693, Train Acc: 0.9756, Prec: 0.9756, Rec: 0.9758, F1: 0.9757 | Val Loss: 4.4341, Val Acc: 0.6035, Prec: 0.3466, Rec: 0.3382, F1: 0.3400\nEpoch 389/500: Train Loss: 0.0759, Train Acc: 0.9730, Prec: 0.9730, Rec: 0.9733, F1: 0.9731 | Val Loss: 4.2652, Val Acc: 0.5930, Prec: 0.3438, Rec: 0.3487, F1: 0.3444\nEpoch 390/500: Train Loss: 0.0771, Train Acc: 0.9754, Prec: 0.9754, Rec: 0.9757, F1: 0.9755 | Val Loss: 4.2954, Val Acc: 0.5860, Prec: 0.3464, Rec: 0.3434, F1: 0.3423\nEpoch 391/500: Train Loss: 0.0674, Train Acc: 0.9770, Prec: 0.9770, Rec: 0.9773, F1: 0.9771 | Val Loss: 4.1207, Val Acc: 0.6316, Prec: 0.3866, Rec: 0.3822, F1: 0.3839\nEpoch 392/500: Train Loss: 0.0674, Train Acc: 0.9760, Prec: 0.9760, Rec: 0.9763, F1: 0.9761 | Val Loss: 4.4176, Val Acc: 0.6316, Prec: 0.3572, Rec: 0.3400, F1: 0.3468\nEpoch 393/500: Train Loss: 0.0770, Train Acc: 0.9722, Prec: 0.9722, Rec: 0.9725, F1: 0.9723 | Val Loss: 4.2279, Val Acc: 0.5965, Prec: 0.3202, Rec: 0.3254, F1: 0.3221\nEpoch 394/500: Train Loss: 0.0771, Train Acc: 0.9724, Prec: 0.9725, Rec: 0.9727, F1: 0.9725 | Val Loss: 4.4217, Val Acc: 0.6105, Prec: 0.3545, Rec: 0.3474, F1: 0.3496\nEpoch 395/500: Train Loss: 0.0638, Train Acc: 0.9766, Prec: 0.9767, Rec: 0.9769, F1: 0.9767 | Val Loss: 4.3658, Val Acc: 0.6175, Prec: 0.3599, Rec: 0.3492, F1: 0.3489\nEpoch 396/500: Train Loss: 0.0682, Train Acc: 0.9760, Prec: 0.9760, Rec: 0.9764, F1: 0.9761 | Val Loss: 4.2352, Val Acc: 0.6000, Prec: 0.3232, Rec: 0.3192, F1: 0.3195\nEpoch 397/500: Train Loss: 0.0673, Train Acc: 0.9766, Prec: 0.9767, Rec: 0.9769, F1: 0.9767 | Val Loss: 4.2859, Val Acc: 0.6000, Prec: 0.3596, Rec: 0.3614, F1: 0.3563\nEpoch 398/500: Train Loss: 0.0691, Train Acc: 0.9758, Prec: 0.9759, Rec: 0.9762, F1: 0.9760 | Val Loss: 4.3060, Val Acc: 0.6175, Prec: 0.3687, Rec: 0.3495, F1: 0.3552\nEpoch 399/500: Train Loss: 0.0665, Train Acc: 0.9750, Prec: 0.9751, Rec: 0.9753, F1: 0.9752 | Val Loss: 4.1539, Val Acc: 0.5860, Prec: 0.3256, Rec: 0.3151, F1: 0.3189\nEpoch 400/500: Train Loss: 0.0771, Train Acc: 0.9710, Prec: 0.9711, Rec: 0.9713, F1: 0.9711 | Val Loss: 4.2392, Val Acc: 0.6140, Prec: 0.3688, Rec: 0.3554, F1: 0.3569\nEpoch 401/500: Train Loss: 0.0741, Train Acc: 0.9766, Prec: 0.9766, Rec: 0.9769, F1: 0.9767 | Val Loss: 4.4584, Val Acc: 0.5965, Prec: 0.3207, Rec: 0.3081, F1: 0.3131\nEpoch 402/500: Train Loss: 0.0685, Train Acc: 0.9768, Prec: 0.9769, Rec: 0.9771, F1: 0.9769 | Val Loss: 4.4538, Val Acc: 0.6105, Prec: 0.3388, Rec: 0.3367, F1: 0.3360\nEpoch 403/500: Train Loss: 0.0773, Train Acc: 0.9708, Prec: 0.9708, Rec: 0.9711, F1: 0.9709 | Val Loss: 4.3060, Val Acc: 0.6000, Prec: 0.3096, Rec: 0.3119, F1: 0.3087\nEpoch 404/500: Train Loss: 0.0648, Train Acc: 0.9780, Prec: 0.9781, Rec: 0.9783, F1: 0.9781 | Val Loss: 4.3433, Val Acc: 0.6035, Prec: 0.3381, Rec: 0.3275, F1: 0.3296\nEpoch 405/500: Train Loss: 0.0762, Train Acc: 0.9708, Prec: 0.9708, Rec: 0.9712, F1: 0.9709 | Val Loss: 4.3827, Val Acc: 0.6105, Prec: 0.3409, Rec: 0.3331, F1: 0.3346\nEpoch 406/500: Train Loss: 0.0675, Train Acc: 0.9760, Prec: 0.9761, Rec: 0.9763, F1: 0.9761 | Val Loss: 4.4860, Val Acc: 0.6351, Prec: 0.3771, Rec: 0.3522, F1: 0.3618\nEpoch 407/500: Train Loss: 0.0731, Train Acc: 0.9758, Prec: 0.9758, Rec: 0.9761, F1: 0.9759 | Val Loss: 4.2740, Val Acc: 0.6386, Prec: 0.3475, Rec: 0.3312, F1: 0.3356\nEpoch 408/500: Train Loss: 0.0579, Train Acc: 0.9800, Prec: 0.9801, Rec: 0.9802, F1: 0.9801 | Val Loss: 4.3240, Val Acc: 0.5895, Prec: 0.3149, Rec: 0.3124, F1: 0.3126\nEpoch 409/500: Train Loss: 0.0669, Train Acc: 0.9744, Prec: 0.9744, Rec: 0.9747, F1: 0.9745 | Val Loss: 4.3204, Val Acc: 0.6105, Prec: 0.3723, Rec: 0.3719, F1: 0.3717\nEpoch 410/500: Train Loss: 0.0684, Train Acc: 0.9744, Prec: 0.9744, Rec: 0.9747, F1: 0.9745 | Val Loss: 4.3240, Val Acc: 0.5860, Prec: 0.3400, Rec: 0.3397, F1: 0.3398\nEpoch 411/500: Train Loss: 0.0756, Train Acc: 0.9716, Prec: 0.9716, Rec: 0.9719, F1: 0.9717 | Val Loss: 4.3766, Val Acc: 0.6246, Prec: 0.3590, Rec: 0.3480, F1: 0.3482\nEpoch 412/500: Train Loss: 0.0685, Train Acc: 0.9750, Prec: 0.9750, Rec: 0.9754, F1: 0.9751 | Val Loss: 4.4714, Val Acc: 0.6140, Prec: 0.3379, Rec: 0.3239, F1: 0.3265\nEpoch 413/500: Train Loss: 0.0569, Train Acc: 0.9802, Prec: 0.9803, Rec: 0.9805, F1: 0.9803 | Val Loss: 4.5823, Val Acc: 0.6070, Prec: 0.3386, Rec: 0.3252, F1: 0.3283\nEpoch 414/500: Train Loss: 0.0506, Train Acc: 0.9826, Prec: 0.9826, Rec: 0.9829, F1: 0.9827 | Val Loss: 4.5499, Val Acc: 0.5719, Prec: 0.3049, Rec: 0.3170, F1: 0.3076\nEpoch 415/500: Train Loss: 0.0661, Train Acc: 0.9782, Prec: 0.9782, Rec: 0.9784, F1: 0.9783 | Val Loss: 4.3545, Val Acc: 0.5930, Prec: 0.3203, Rec: 0.3135, F1: 0.3142\nEpoch 416/500: Train Loss: 0.0655, Train Acc: 0.9748, Prec: 0.9749, Rec: 0.9751, F1: 0.9750 | Val Loss: 4.2421, Val Acc: 0.6070, Prec: 0.3546, Rec: 0.3531, F1: 0.3525\nEpoch 417/500: Train Loss: 0.0778, Train Acc: 0.9726, Prec: 0.9726, Rec: 0.9729, F1: 0.9727 | Val Loss: 4.2558, Val Acc: 0.6105, Prec: 0.3731, Rec: 0.3619, F1: 0.3661\nEpoch 418/500: Train Loss: 0.0653, Train Acc: 0.9778, Prec: 0.9779, Rec: 0.9780, F1: 0.9779 | Val Loss: 4.5022, Val Acc: 0.6140, Prec: 0.3364, Rec: 0.3305, F1: 0.3319\nEpoch 419/500: Train Loss: 0.0664, Train Acc: 0.9768, Prec: 0.9768, Rec: 0.9771, F1: 0.9769 | Val Loss: 4.3507, Val Acc: 0.5825, Prec: 0.3188, Rec: 0.3241, F1: 0.3202\nEpoch 420/500: Train Loss: 0.0766, Train Acc: 0.9728, Prec: 0.9729, Rec: 0.9731, F1: 0.9729 | Val Loss: 4.4895, Val Acc: 0.6140, Prec: 0.3594, Rec: 0.3587, F1: 0.3577\nEpoch 421/500: Train Loss: 0.0605, Train Acc: 0.9804, Prec: 0.9805, Rec: 0.9807, F1: 0.9805 | Val Loss: 4.3323, Val Acc: 0.5965, Prec: 0.3451, Rec: 0.3433, F1: 0.3439\nEpoch 422/500: Train Loss: 0.0713, Train Acc: 0.9736, Prec: 0.9737, Rec: 0.9740, F1: 0.9737 | Val Loss: 4.4002, Val Acc: 0.5965, Prec: 0.3387, Rec: 0.3360, F1: 0.3371\nEpoch 423/500: Train Loss: 0.0601, Train Acc: 0.9786, Prec: 0.9787, Rec: 0.9789, F1: 0.9787 | Val Loss: 4.3899, Val Acc: 0.6070, Prec: 0.3362, Rec: 0.3255, F1: 0.3297\nEpoch 424/500: Train Loss: 0.0641, Train Acc: 0.9754, Prec: 0.9755, Rec: 0.9758, F1: 0.9756 | Val Loss: 4.3964, Val Acc: 0.6140, Prec: 0.3442, Rec: 0.3380, F1: 0.3398\nEpoch 425/500: Train Loss: 0.0702, Train Acc: 0.9754, Prec: 0.9754, Rec: 0.9758, F1: 0.9755 | Val Loss: 4.4695, Val Acc: 0.6175, Prec: 0.3489, Rec: 0.3391, F1: 0.3405\nEpoch 426/500: Train Loss: 0.0591, Train Acc: 0.9780, Prec: 0.9780, Rec: 0.9783, F1: 0.9781 | Val Loss: 4.5487, Val Acc: 0.6281, Prec: 0.3610, Rec: 0.3248, F1: 0.3343\nEpoch 427/500: Train Loss: 0.0642, Train Acc: 0.9780, Prec: 0.9780, Rec: 0.9783, F1: 0.9781 | Val Loss: 4.3957, Val Acc: 0.6070, Prec: 0.3473, Rec: 0.3393, F1: 0.3413\nEpoch 428/500: Train Loss: 0.0610, Train Acc: 0.9786, Prec: 0.9786, Rec: 0.9789, F1: 0.9787 | Val Loss: 4.5525, Val Acc: 0.6246, Prec: 0.3696, Rec: 0.3583, F1: 0.3544\nEpoch 429/500: Train Loss: 0.0641, Train Acc: 0.9792, Prec: 0.9792, Rec: 0.9795, F1: 0.9793 | Val Loss: 4.4687, Val Acc: 0.6140, Prec: 0.3486, Rec: 0.3481, F1: 0.3460\nEpoch 430/500: Train Loss: 0.0633, Train Acc: 0.9758, Prec: 0.9758, Rec: 0.9761, F1: 0.9759 | Val Loss: 4.4378, Val Acc: 0.6140, Prec: 0.3784, Rec: 0.3733, F1: 0.3736\nEpoch 431/500: Train Loss: 0.0688, Train Acc: 0.9748, Prec: 0.9749, Rec: 0.9750, F1: 0.9749 | Val Loss: 4.5076, Val Acc: 0.6000, Prec: 0.3554, Rec: 0.3510, F1: 0.3497\nEpoch 432/500: Train Loss: 0.0693, Train Acc: 0.9766, Prec: 0.9767, Rec: 0.9769, F1: 0.9767 | Val Loss: 4.4133, Val Acc: 0.6386, Prec: 0.3755, Rec: 0.3596, F1: 0.3626\nEpoch 433/500: Train Loss: 0.0576, Train Acc: 0.9790, Prec: 0.9790, Rec: 0.9793, F1: 0.9791 | Val Loss: 4.5363, Val Acc: 0.5754, Prec: 0.3330, Rec: 0.3361, F1: 0.3336\nEpoch 434/500: Train Loss: 0.0559, Train Acc: 0.9782, Prec: 0.9783, Rec: 0.9785, F1: 0.9783 | Val Loss: 4.3696, Val Acc: 0.6070, Prec: 0.3671, Rec: 0.3535, F1: 0.3585\nEpoch 435/500: Train Loss: 0.0659, Train Acc: 0.9762, Prec: 0.9762, Rec: 0.9765, F1: 0.9763 | Val Loss: 4.5116, Val Acc: 0.6070, Prec: 0.3614, Rec: 0.3598, F1: 0.3532\nEpoch 436/500: Train Loss: 0.0674, Train Acc: 0.9766, Prec: 0.9766, Rec: 0.9769, F1: 0.9767 | Val Loss: 4.3633, Val Acc: 0.6000, Prec: 0.3293, Rec: 0.3226, F1: 0.3240\nEpoch 437/500: Train Loss: 0.0667, Train Acc: 0.9766, Prec: 0.9767, Rec: 0.9768, F1: 0.9767 | Val Loss: 4.5490, Val Acc: 0.6035, Prec: 0.3477, Rec: 0.3449, F1: 0.3460\nEpoch 438/500: Train Loss: 0.0605, Train Acc: 0.9776, Prec: 0.9777, Rec: 0.9779, F1: 0.9777 | Val Loss: 4.3017, Val Acc: 0.6140, Prec: 0.3619, Rec: 0.3485, F1: 0.3536\nEpoch 439/500: Train Loss: 0.0734, Train Acc: 0.9744, Prec: 0.9745, Rec: 0.9747, F1: 0.9745 | Val Loss: 4.4181, Val Acc: 0.6000, Prec: 0.3333, Rec: 0.3469, F1: 0.3388\nEpoch 440/500: Train Loss: 0.0539, Train Acc: 0.9810, Prec: 0.9811, Rec: 0.9812, F1: 0.9811 | Val Loss: 4.6173, Val Acc: 0.5825, Prec: 0.3441, Rec: 0.3490, F1: 0.3463\nEpoch 441/500: Train Loss: 0.0623, Train Acc: 0.9772, Prec: 0.9773, Rec: 0.9775, F1: 0.9773 | Val Loss: 4.3745, Val Acc: 0.6211, Prec: 0.3789, Rec: 0.3548, F1: 0.3639\nEpoch 442/500: Train Loss: 0.0699, Train Acc: 0.9736, Prec: 0.9737, Rec: 0.9739, F1: 0.9737 | Val Loss: 4.3593, Val Acc: 0.6000, Prec: 0.3309, Rec: 0.3195, F1: 0.3236\nEpoch 443/500: Train Loss: 0.0571, Train Acc: 0.9816, Prec: 0.9816, Rec: 0.9818, F1: 0.9817 | Val Loss: 4.4749, Val Acc: 0.6035, Prec: 0.3691, Rec: 0.3597, F1: 0.3633\nEpoch 444/500: Train Loss: 0.0640, Train Acc: 0.9776, Prec: 0.9776, Rec: 0.9779, F1: 0.9777 | Val Loss: 4.5207, Val Acc: 0.5965, Prec: 0.3273, Rec: 0.3250, F1: 0.3232\nEpoch 445/500: Train Loss: 0.0682, Train Acc: 0.9742, Prec: 0.9742, Rec: 0.9744, F1: 0.9743 | Val Loss: 4.5395, Val Acc: 0.5754, Prec: 0.3178, Rec: 0.3153, F1: 0.3153\nEpoch 446/500: Train Loss: 0.0729, Train Acc: 0.9764, Prec: 0.9764, Rec: 0.9767, F1: 0.9765 | Val Loss: 4.4435, Val Acc: 0.5930, Prec: 0.3324, Rec: 0.3246, F1: 0.3277\nEpoch 447/500: Train Loss: 0.0593, Train Acc: 0.9794, Prec: 0.9795, Rec: 0.9796, F1: 0.9795 | Val Loss: 4.3964, Val Acc: 0.6175, Prec: 0.3713, Rec: 0.3602, F1: 0.3649\nEpoch 448/500: Train Loss: 0.0627, Train Acc: 0.9784, Prec: 0.9785, Rec: 0.9787, F1: 0.9785 | Val Loss: 4.4747, Val Acc: 0.6070, Prec: 0.3577, Rec: 0.3534, F1: 0.3549\nEpoch 449/500: Train Loss: 0.0742, Train Acc: 0.9732, Prec: 0.9732, Rec: 0.9735, F1: 0.9733 | Val Loss: 4.4501, Val Acc: 0.5965, Prec: 0.3339, Rec: 0.3354, F1: 0.3337\nEpoch 450/500: Train Loss: 0.0531, Train Acc: 0.9810, Prec: 0.9811, Rec: 0.9813, F1: 0.9811 | Val Loss: 4.4278, Val Acc: 0.6351, Prec: 0.3843, Rec: 0.3550, F1: 0.3624\nEpoch 451/500: Train Loss: 0.0478, Train Acc: 0.9840, Prec: 0.9840, Rec: 0.9842, F1: 0.9841 | Val Loss: 4.3490, Val Acc: 0.5930, Prec: 0.3352, Rec: 0.3346, F1: 0.3333\nEpoch 452/500: Train Loss: 0.0723, Train Acc: 0.9750, Prec: 0.9751, Rec: 0.9753, F1: 0.9751 | Val Loss: 4.3564, Val Acc: 0.6000, Prec: 0.3633, Rec: 0.3683, F1: 0.3635\nEpoch 453/500: Train Loss: 0.0554, Train Acc: 0.9808, Prec: 0.9808, Rec: 0.9811, F1: 0.9809 | Val Loss: 4.5345, Val Acc: 0.6140, Prec: 0.3740, Rec: 0.3664, F1: 0.3697\nEpoch 454/500: Train Loss: 0.0827, Train Acc: 0.9708, Prec: 0.9708, Rec: 0.9712, F1: 0.9709 | Val Loss: 4.6374, Val Acc: 0.6000, Prec: 0.3744, Rec: 0.3689, F1: 0.3709\nEpoch 455/500: Train Loss: 0.0668, Train Acc: 0.9768, Prec: 0.9769, Rec: 0.9772, F1: 0.9769 | Val Loss: 4.4500, Val Acc: 0.6035, Prec: 0.3797, Rec: 0.3735, F1: 0.3752\nEpoch 456/500: Train Loss: 0.0555, Train Acc: 0.9808, Prec: 0.9809, Rec: 0.9810, F1: 0.9809 | Val Loss: 4.3836, Val Acc: 0.6070, Prec: 0.3615, Rec: 0.3532, F1: 0.3565\nEpoch 457/500: Train Loss: 0.0502, Train Acc: 0.9812, Prec: 0.9813, Rec: 0.9815, F1: 0.9813 | Val Loss: 4.5981, Val Acc: 0.6000, Prec: 0.3294, Rec: 0.3192, F1: 0.3194\nEpoch 458/500: Train Loss: 0.0656, Train Acc: 0.9794, Prec: 0.9795, Rec: 0.9796, F1: 0.9795 | Val Loss: 4.5479, Val Acc: 0.6456, Prec: 0.3809, Rec: 0.3621, F1: 0.3674\nEpoch 459/500: Train Loss: 0.0550, Train Acc: 0.9798, Prec: 0.9798, Rec: 0.9801, F1: 0.9799 | Val Loss: 4.6200, Val Acc: 0.6000, Prec: 0.3686, Rec: 0.3648, F1: 0.3643\nEpoch 460/500: Train Loss: 0.0582, Train Acc: 0.9770, Prec: 0.9771, Rec: 0.9773, F1: 0.9771 | Val Loss: 4.6860, Val Acc: 0.5930, Prec: 0.3629, Rec: 0.3592, F1: 0.3575\nEpoch 461/500: Train Loss: 0.0628, Train Acc: 0.9754, Prec: 0.9754, Rec: 0.9757, F1: 0.9755 | Val Loss: 4.6619, Val Acc: 0.6211, Prec: 0.3704, Rec: 0.3679, F1: 0.3681\nEpoch 462/500: Train Loss: 0.0744, Train Acc: 0.9734, Prec: 0.9735, Rec: 0.9737, F1: 0.9735 | Val Loss: 4.3737, Val Acc: 0.6140, Prec: 0.3571, Rec: 0.3446, F1: 0.3494\nEpoch 463/500: Train Loss: 0.0637, Train Acc: 0.9790, Prec: 0.9790, Rec: 0.9793, F1: 0.9791 | Val Loss: 4.5628, Val Acc: 0.6281, Prec: 0.3808, Rec: 0.3667, F1: 0.3722\nEpoch 464/500: Train Loss: 0.0566, Train Acc: 0.9800, Prec: 0.9800, Rec: 0.9802, F1: 0.9801 | Val Loss: 4.5067, Val Acc: 0.5930, Prec: 0.3481, Rec: 0.3450, F1: 0.3460\nEpoch 465/500: Train Loss: 0.0583, Train Acc: 0.9794, Prec: 0.9794, Rec: 0.9797, F1: 0.9795 | Val Loss: 4.5019, Val Acc: 0.6035, Prec: 0.3803, Rec: 0.3669, F1: 0.3708\nEpoch 466/500: Train Loss: 0.0577, Train Acc: 0.9808, Prec: 0.9809, Rec: 0.9810, F1: 0.9809 | Val Loss: 4.6894, Val Acc: 0.5930, Prec: 0.3653, Rec: 0.3529, F1: 0.3569\nEpoch 467/500: Train Loss: 0.0532, Train Acc: 0.9828, Prec: 0.9828, Rec: 0.9830, F1: 0.9829 | Val Loss: 4.5459, Val Acc: 0.5930, Prec: 0.3664, Rec: 0.3592, F1: 0.3599\nEpoch 468/500: Train Loss: 0.0525, Train Acc: 0.9798, Prec: 0.9799, Rec: 0.9801, F1: 0.9799 | Val Loss: 4.5521, Val Acc: 0.5965, Prec: 0.3608, Rec: 0.3603, F1: 0.3601\nEpoch 469/500: Train Loss: 0.0732, Train Acc: 0.9728, Prec: 0.9728, Rec: 0.9731, F1: 0.9729 | Val Loss: 4.3745, Val Acc: 0.5649, Prec: 0.3379, Rec: 0.3363, F1: 0.3347\nEpoch 470/500: Train Loss: 0.0668, Train Acc: 0.9760, Prec: 0.9760, Rec: 0.9762, F1: 0.9761 | Val Loss: 4.3476, Val Acc: 0.6105, Prec: 0.3500, Rec: 0.3539, F1: 0.3512\nEpoch 471/500: Train Loss: 0.0537, Train Acc: 0.9804, Prec: 0.9804, Rec: 0.9806, F1: 0.9805 | Val Loss: 4.6069, Val Acc: 0.6070, Prec: 0.3383, Rec: 0.3425, F1: 0.3399\nEpoch 472/500: Train Loss: 0.0601, Train Acc: 0.9806, Prec: 0.9806, Rec: 0.9808, F1: 0.9807 | Val Loss: 4.6076, Val Acc: 0.5895, Prec: 0.3436, Rec: 0.3405, F1: 0.3409\nEpoch 473/500: Train Loss: 0.0564, Train Acc: 0.9808, Prec: 0.9808, Rec: 0.9810, F1: 0.9809 | Val Loss: 4.5969, Val Acc: 0.5930, Prec: 0.3188, Rec: 0.3132, F1: 0.3144\nEpoch 474/500: Train Loss: 0.0643, Train Acc: 0.9768, Prec: 0.9769, Rec: 0.9771, F1: 0.9769 | Val Loss: 4.5956, Val Acc: 0.5965, Prec: 0.3303, Rec: 0.3253, F1: 0.3235\nEpoch 475/500: Train Loss: 0.0642, Train Acc: 0.9770, Prec: 0.9770, Rec: 0.9773, F1: 0.9771 | Val Loss: 4.7307, Val Acc: 0.6211, Prec: 0.3660, Rec: 0.3506, F1: 0.3558\nEpoch 476/500: Train Loss: 0.0680, Train Acc: 0.9754, Prec: 0.9754, Rec: 0.9757, F1: 0.9755 | Val Loss: 4.5121, Val Acc: 0.6105, Prec: 0.3328, Rec: 0.3329, F1: 0.3312\nEpoch 477/500: Train Loss: 0.0652, Train Acc: 0.9778, Prec: 0.9778, Rec: 0.9781, F1: 0.9779 | Val Loss: 4.4927, Val Acc: 0.5965, Prec: 0.3513, Rec: 0.3429, F1: 0.3459\nEpoch 478/500: Train Loss: 0.0589, Train Acc: 0.9806, Prec: 0.9806, Rec: 0.9809, F1: 0.9807 | Val Loss: 4.6218, Val Acc: 0.6246, Prec: 0.3898, Rec: 0.3869, F1: 0.3882\nBest model saved with F1: 0.3882 at epoch 478\nEpoch 479/500: Train Loss: 0.0515, Train Acc: 0.9826, Prec: 0.9826, Rec: 0.9829, F1: 0.9827 | Val Loss: 4.6968, Val Acc: 0.6140, Prec: 0.3763, Rec: 0.3764, F1: 0.3762\nEpoch 480/500: Train Loss: 0.0549, Train Acc: 0.9804, Prec: 0.9804, Rec: 0.9807, F1: 0.9805 | Val Loss: 4.8207, Val Acc: 0.6211, Prec: 0.3739, Rec: 0.3361, F1: 0.3400\nEpoch 481/500: Train Loss: 0.0737, Train Acc: 0.9732, Prec: 0.9732, Rec: 0.9736, F1: 0.9733 | Val Loss: 4.6444, Val Acc: 0.6316, Prec: 0.3659, Rec: 0.3501, F1: 0.3560\nEpoch 482/500: Train Loss: 0.0684, Train Acc: 0.9758, Prec: 0.9758, Rec: 0.9760, F1: 0.9759 | Val Loss: 4.7235, Val Acc: 0.6351, Prec: 0.3900, Rec: 0.3584, F1: 0.3666\nEpoch 483/500: Train Loss: 0.0636, Train Acc: 0.9766, Prec: 0.9766, Rec: 0.9769, F1: 0.9767 | Val Loss: 4.5164, Val Acc: 0.6070, Prec: 0.3525, Rec: 0.3428, F1: 0.3459\nEpoch 484/500: Train Loss: 0.0575, Train Acc: 0.9818, Prec: 0.9818, Rec: 0.9820, F1: 0.9819 | Val Loss: 4.4401, Val Acc: 0.6351, Prec: 0.3994, Rec: 0.3833, F1: 0.3885\nBest model saved with F1: 0.3885 at epoch 484\nEpoch 485/500: Train Loss: 0.0605, Train Acc: 0.9782, Prec: 0.9782, Rec: 0.9784, F1: 0.9783 | Val Loss: 4.4450, Val Acc: 0.6421, Prec: 0.4124, Rec: 0.4031, F1: 0.4068\nBest model saved with F1: 0.4068 at epoch 485\nEpoch 486/500: Train Loss: 0.0636, Train Acc: 0.9770, Prec: 0.9771, Rec: 0.9773, F1: 0.9771 | Val Loss: 4.5487, Val Acc: 0.5930, Prec: 0.3346, Rec: 0.3246, F1: 0.3280\nEpoch 487/500: Train Loss: 0.0574, Train Acc: 0.9808, Prec: 0.9808, Rec: 0.9811, F1: 0.9809 | Val Loss: 4.5738, Val Acc: 0.6105, Prec: 0.3694, Rec: 0.3646, F1: 0.3661\nEpoch 488/500: Train Loss: 0.0621, Train Acc: 0.9768, Prec: 0.9768, Rec: 0.9772, F1: 0.9769 | Val Loss: 4.6978, Val Acc: 0.5965, Prec: 0.3472, Rec: 0.3496, F1: 0.3469\nEpoch 489/500: Train Loss: 0.0662, Train Acc: 0.9754, Prec: 0.9755, Rec: 0.9757, F1: 0.9755 | Val Loss: 4.7639, Val Acc: 0.6140, Prec: 0.3435, Rec: 0.3339, F1: 0.3364\nEpoch 490/500: Train Loss: 0.0454, Train Acc: 0.9832, Prec: 0.9833, Rec: 0.9834, F1: 0.9833 | Val Loss: 4.7087, Val Acc: 0.6456, Prec: 0.4113, Rec: 0.3794, F1: 0.3917\nEpoch 491/500: Train Loss: 0.0601, Train Acc: 0.9798, Prec: 0.9798, Rec: 0.9800, F1: 0.9799 | Val Loss: 4.6179, Val Acc: 0.5895, Prec: 0.3505, Rec: 0.3439, F1: 0.3404\nEpoch 492/500: Train Loss: 0.0592, Train Acc: 0.9782, Prec: 0.9783, Rec: 0.9785, F1: 0.9783 | Val Loss: 4.6244, Val Acc: 0.6140, Prec: 0.3674, Rec: 0.3692, F1: 0.3667\nEpoch 493/500: Train Loss: 0.0560, Train Acc: 0.9812, Prec: 0.9812, Rec: 0.9814, F1: 0.9812 | Val Loss: 4.5426, Val Acc: 0.5860, Prec: 0.3215, Rec: 0.3183, F1: 0.3175\nEpoch 494/500: Train Loss: 0.0616, Train Acc: 0.9756, Prec: 0.9757, Rec: 0.9758, F1: 0.9757 | Val Loss: 4.5109, Val Acc: 0.5965, Prec: 0.3417, Rec: 0.3391, F1: 0.3399\nEpoch 495/500: Train Loss: 0.0540, Train Acc: 0.9822, Prec: 0.9822, Rec: 0.9825, F1: 0.9823 | Val Loss: 4.9242, Val Acc: 0.6211, Prec: 0.3321, Rec: 0.3154, F1: 0.3198\nEpoch 496/500: Train Loss: 0.0443, Train Acc: 0.9858, Prec: 0.9859, Rec: 0.9860, F1: 0.9859 | Val Loss: 4.6485, Val Acc: 0.6211, Prec: 0.3640, Rec: 0.3609, F1: 0.3599\nEpoch 497/500: Train Loss: 0.0517, Train Acc: 0.9824, Prec: 0.9825, Rec: 0.9826, F1: 0.9825 | Val Loss: 4.4369, Val Acc: 0.6000, Prec: 0.3404, Rec: 0.3333, F1: 0.3350\nEpoch 498/500: Train Loss: 0.0697, Train Acc: 0.9794, Prec: 0.9794, Rec: 0.9797, F1: 0.9795 | Val Loss: 4.6192, Val Acc: 0.6316, Prec: 0.3823, Rec: 0.3608, F1: 0.3632\nEpoch 499/500: Train Loss: 0.0559, Train Acc: 0.9808, Prec: 0.9809, Rec: 0.9811, F1: 0.9810 | Val Loss: 4.7009, Val Acc: 0.6035, Prec: 0.3281, Rec: 0.3241, F1: 0.3249\nEpoch 500/500: Train Loss: 0.0595, Train Acc: 0.9792, Prec: 0.9792, Rec: 0.9794, F1: 0.9793 | Val Loss: 4.5648, Val Acc: 0.6211, Prec: 0.3436, Rec: 0.3395, F1: 0.3401\nBest model saved at: ./models/best_model_epoch_485_f1_0.4068.pth\n","output_type":"stream"}],"execution_count":44},{"cell_type":"markdown","source":"## Test","metadata":{"id":"tQI1YtBouXDZ"}},{"cell_type":"code","source":"def predict_and_generate_submission(test_loader, best_model_path, submission_file_path):\n    # Load the best model with weights_only=True to avoid security warnings\n    model = MLPModel(input_dim, hidden_dim, output_dim, dropout_p).to(device)\n    model.load_state_dict(torch.load(best_model_path, weights_only=True))\n    model.eval()  # Set the model to evaluation mode\n\n    test_predictions = []\n    with torch.no_grad():\n        for inputs in test_loader:\n            # Ensure inputs are converted to a tensor and stacked into a batch if necessary\n            if isinstance(inputs, list):\n                # Convert each item to tensor using .detach() to avoid the user warning\n                inputs = [i.clone().detach().to(device) if isinstance(i, torch.Tensor) else torch.tensor(i).to(device) for i in inputs]\n                inputs = torch.stack(inputs)  # Stack them into a batch tensor\n            else:\n                inputs = inputs.to(device)  # If inputs is already a tensor, move it to device\n\n            outputs = model(inputs).squeeze()\n\n            # Predict binary labels\n            _, preds = torch.max(outputs, dim=1)\n            test_predictions.extend(preds.tolist())\n\n    # Prepare the submission DataFrame\n    submission_df = pd.DataFrame({\n        TEXT_VAR: [i for i in test_df[TEXT_VAR]],\n        'predictions': test_predictions\n    })\n\n    # Save the predictions to a CSV file\n    submission_df.to_csv(submission_file_path, index=False)\n    print(f\"Submission file saved to {submission_file_path}\")\n\n    return submission_df","metadata":{"id":"0-tQuWKjuWVC","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:28:33.600629Z","iopub.execute_input":"2025-01-10T10:28:33.601021Z","iopub.status.idle":"2025-01-10T10:28:33.607577Z","shell.execute_reply.started":"2025-01-10T10:28:33.600990Z","shell.execute_reply":"2025-01-10T10:28:33.606657Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"submission_file_path = \"submission.csv\"\nsubmission_df = predict_and_generate_submission(test_loader=test_loader, best_model_path=best_model_path, submission_file_path=submission_file_path)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmRfUjoxxyzA","outputId":"389671cc-9e0e-47c5-f686-5d8a672862ce","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:28:36.357188Z","iopub.execute_input":"2025-01-10T10:28:36.357487Z","iopub.status.idle":"2025-01-10T10:28:36.394239Z","shell.execute_reply.started":"2025-01-10T10:28:36.357463Z","shell.execute_reply":"2025-01-10T10:28:36.393459Z"}},"outputs":[{"name":"stdout","text":"Submission file saved to submission.csv\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"submission_df.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"nMKXQNe6yamA","outputId":"98492dec-b085-4d35-eb8c-08654e2c6643","trusted":true,"execution":{"iopub.status.busy":"2025-01-10T10:28:38.627835Z","iopub.execute_input":"2025-01-10T10:28:38.628162Z","iopub.status.idle":"2025-01-10T10:28:38.636543Z","shell.execute_reply.started":"2025-01-10T10:28:38.628133Z","shell.execute_reply":"2025-01-10T10:28:38.635594Z"}},"outputs":[{"execution_count":47,"output_type":"execute_result","data":{"text/plain":"                                                News  predictions\n0  കേരളത്തില്‍ പുരുഷന്മാര്‍ക്ക് രണ്ട് ഭാര്യമാര്‍ ...            1\n1  പാർട്ടിയുടെ കൊടിക്ക് മഹത്വം ഉണ്ടെന്നും സംരംഭങ്...            0\n2  നവകേരള സദസ്സ്: കാട്ടാക്കട ക്രിസ്ത്യൻ കോളേജ് കവ...            3\n3  ശബരിമലയില്‍ അയ്യപ്പ ഭക്തന്‍റെ തല പോലീസ് അടിച്ച...            0\n4  ബൈക്കുകള്‍ സ്വന്തം ജില്ലയില്‍ മാത്രം ഉപയോഗിക്ക...            0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>News</th>\n      <th>predictions</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>കേരളത്തില്‍ പുരുഷന്മാര്‍ക്ക് രണ്ട് ഭാര്യമാര്‍ ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>പാർട്ടിയുടെ കൊടിക്ക് മഹത്വം ഉണ്ടെന്നും സംരംഭങ്...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>നവകേരള സദസ്സ്: കാട്ടാക്കട ക്രിസ്ത്യൻ കോളേജ് കവ...</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ശബരിമലയില്‍ അയ്യപ്പ ഭക്തന്‍റെ തല പോലീസ് അടിച്ച...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ബൈക്കുകള്‍ സ്വന്തം ജില്ലയില്‍ മാത്രം ഉപയോഗിക്ക...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"","metadata":{"id":"NoiMBHYqh-pU","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"If you use it, cite:\n\n*Azmine Toushik Wasi. (2024). CIOL Presnts Winer ML BootCamp. https://github.com/ciol-researchlab/CIOL-Winter-ML-Bootcamp*\n\n```\n@misc{wasi2024CIOL-WMLB,\n      title={CIOL Presnts Winer ML BootCamp},\n      author={Azmine Toushik Wasi},\n      year={2024},\n      url={https://github.com/ciol-researchlab/CIOL-Winter-ML-Bootcamp},\n}```","metadata":{"id":"mmISbA84iHLM"}}]}